{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a7c2cb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\syq\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem\\ChemBERTa-77M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词器和模型加载成功。\n",
      "Epoch 1/100, Loss: 0.6934843129581876\n",
      "Epoch 2/100, Loss: 0.6917401088608636\n",
      "Epoch 3/100, Loss: 0.6899714536137052\n",
      "Epoch 4/100, Loss: 0.6880824300977919\n",
      "Epoch 5/100, Loss: 0.6860930058691237\n",
      "Epoch 6/100, Loss: 0.6838345130284628\n",
      "Epoch 7/100, Loss: 0.6813848680920072\n",
      "Epoch 8/100, Loss: 0.6782132122251723\n",
      "Epoch 9/100, Loss: 0.6746518545680575\n",
      "Epoch 10/100, Loss: 0.670565042230818\n",
      "Epoch 11/100, Loss: 0.6647962530454\n",
      "Epoch 12/100, Loss: 0.6589555276764764\n",
      "Epoch 13/100, Loss: 0.6508004996511672\n",
      "Epoch 14/100, Loss: 0.6413605345620049\n",
      "Epoch 15/100, Loss: 0.630285402139028\n",
      "Epoch 16/100, Loss: 0.6163467698627048\n",
      "Epoch 17/100, Loss: 0.6010187930530972\n",
      "Epoch 18/100, Loss: 0.5827209419674344\n",
      "Epoch 19/100, Loss: 0.562623467710283\n",
      "Epoch 20/100, Loss: 0.5421025223202176\n",
      "Epoch 21/100, Loss: 0.5208572612868415\n",
      "Epoch 22/100, Loss: 0.4997095399432712\n",
      "Epoch 23/100, Loss: 0.47932864560021293\n",
      "Epoch 24/100, Loss: 0.4631395737330119\n",
      "Epoch 25/100, Loss: 0.44777796003553605\n",
      "Epoch 26/100, Loss: 0.4330372412999471\n",
      "Epoch 27/100, Loss: 0.4193409714433882\n",
      "Epoch 28/100, Loss: 0.40962331162558663\n",
      "Epoch 29/100, Loss: 0.4020484255419837\n",
      "Epoch 30/100, Loss: 0.39547422859403825\n",
      "Epoch 31/100, Loss: 0.3890797131591373\n",
      "Epoch 32/100, Loss: 0.38296493225627476\n",
      "Epoch 33/100, Loss: 0.3799094524648454\n",
      "Epoch 34/100, Loss: 0.3768534693453047\n",
      "Epoch 35/100, Loss: 0.37196164660983616\n",
      "Epoch 36/100, Loss: 0.3682338794072469\n",
      "Epoch 37/100, Loss: 0.3652588625748952\n",
      "Epoch 38/100, Loss: 0.3645358979701996\n",
      "Epoch 39/100, Loss: 0.36148930258221096\n",
      "Epoch 40/100, Loss: 0.35975875125990975\n",
      "Epoch 41/100, Loss: 0.3583318259980943\n",
      "Epoch 42/100, Loss: 0.3583149181471931\n",
      "Epoch 43/100, Loss: 0.35577768749660915\n",
      "Epoch 44/100, Loss: 0.35313377446598476\n",
      "Epoch 45/100, Loss: 0.35345714622073704\n",
      "Epoch 46/100, Loss: 0.35263731744554305\n",
      "Epoch 47/100, Loss: 0.3503236439492967\n",
      "Epoch 48/100, Loss: 0.3533262875345018\n",
      "Epoch 49/100, Loss: 0.3495257364379035\n",
      "Epoch 50/100, Loss: 0.34840816921657985\n",
      "Epoch 51/100, Loss: 0.3471812042925093\n",
      "Epoch 52/100, Loss: 0.345968892176946\n",
      "Epoch 53/100, Loss: 0.34710466861724854\n",
      "Epoch 54/100, Loss: 0.3460291624069214\n",
      "Epoch 55/100, Loss: 0.3457217150264316\n",
      "Epoch 56/100, Loss: 0.3449230160978105\n",
      "Epoch 57/100, Loss: 0.34163423048125374\n",
      "Epoch 58/100, Loss: 0.34295055601331925\n",
      "Epoch 59/100, Loss: 0.33987881739934284\n",
      "Epoch 60/100, Loss: 0.3389630715052287\n",
      "Epoch 61/100, Loss: 0.3390481405787998\n",
      "Epoch 62/100, Loss: 0.34003255764643353\n",
      "Epoch 63/100, Loss: 0.3376622133784824\n",
      "Epoch 64/100, Loss: 0.3355691730976105\n",
      "Epoch 65/100, Loss: 0.33511580030123395\n",
      "Epoch 66/100, Loss: 0.33532191647423637\n",
      "Epoch 67/100, Loss: 0.3325284553898705\n",
      "Epoch 68/100, Loss: 0.33146997623973423\n",
      "Epoch 69/100, Loss: 0.33329035176171196\n",
      "Epoch 70/100, Loss: 0.33178432120217216\n",
      "Epoch 71/100, Loss: 0.32987262143029106\n",
      "Epoch 72/100, Loss: 0.33251555429564583\n",
      "Epoch 73/100, Loss: 0.32842761940426296\n",
      "Epoch 74/100, Loss: 0.32716474268171525\n",
      "Epoch 75/100, Loss: 0.32612571782535976\n",
      "Epoch 76/100, Loss: 0.3258460693889194\n",
      "Epoch 77/100, Loss: 0.32340092129177517\n",
      "Epoch 78/100, Loss: 0.32343998551368713\n",
      "Epoch 79/100, Loss: 0.32369429535335964\n",
      "Epoch 80/100, Loss: 0.3244892292552524\n",
      "Epoch 81/100, Loss: 0.32168091668023\n",
      "Epoch 82/100, Loss: 0.3212127950456407\n",
      "Epoch 83/100, Loss: 0.3216293719079759\n",
      "Epoch 84/100, Loss: 0.31742936703893876\n",
      "Epoch 85/100, Loss: 0.31875408358044094\n",
      "Epoch 86/100, Loss: 0.3180520534515381\n",
      "Epoch 87/100, Loss: 0.31706731849246556\n",
      "Epoch 88/100, Loss: 0.31768789225154453\n",
      "Epoch 89/100, Loss: 0.31536902983983356\n",
      "Epoch 90/100, Loss: 0.31407196323076886\n",
      "Epoch 91/100, Loss: 0.3141205443276299\n",
      "Epoch 92/100, Loss: 0.3143339653809865\n",
      "Epoch 93/100, Loss: 0.3124663101302253\n",
      "Epoch 94/100, Loss: 0.3151158293088277\n",
      "Epoch 95/100, Loss: 0.3082823355992635\n",
      "Epoch 96/100, Loss: 0.3103020356761085\n",
      "Epoch 97/100, Loss: 0.31219461891386246\n",
      "Epoch 98/100, Loss: 0.3115825454394023\n",
      "Epoch 99/100, Loss: 0.30784549646907383\n",
      "Epoch 100/100, Loss: 0.30695567859543693\n",
      "增强的嵌入表示和预测值获取成功。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "# 加载多任务数据集\n",
    "file_path = 'cti.csv'\n",
    "multi_task_df = pd.read_csv(file_path)\n",
    "\n",
    "# 检查和处理空值\n",
    "multi_task_df.iloc[:, 1:] = multi_task_df.iloc[:, 1:].apply(pd.to_numeric, errors='coerce')  # 将非数值数据转换为NaN\n",
    "labels = multi_task_df.iloc[:, 1:].values\n",
    "\n",
    "# 创建标签掩码，标记非空值的位置\n",
    "label_masks = ~pd.isna(labels)\n",
    "labels = np.where(pd.isna(labels), -1, labels)  # 用-1填充NaN值\n",
    "labels = labels.astype(float)  # 确保标签数据是浮点数类型\n",
    "labels = torch.tensor(labels, dtype=torch.float)\n",
    "label_masks = torch.tensor(label_masks, dtype=torch.float)\n",
    "\n",
    "# 指定本地模型路径\n",
    "model_path = \"DeepChem\\ChemBERTa-77M-MTR\"\n",
    "\n",
    "# 从本地路径加载分词器和模型配置\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "config.num_labels = labels.shape[1]  # 动态确定标签数量\n",
    "config.output_hidden_states = True\n",
    "\n",
    "# 从本地路径加载具有更新配置的模型\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, config=config)\n",
    "\n",
    "print(\"分词器和模型加载成功。\")\n",
    "\n",
    "# 标记化输入数据\n",
    "inputs = tokenizer(list(multi_task_df.iloc[:, 0]), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "inputs ={key:val.to(device) for key,val in inputs.items()}\n",
    "labels =labels.to(device)\n",
    "label_masks=label_masks.to(device)\n",
    "\n",
    "# 创建数据集对象\n",
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, inputs, labels, label_masks):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        self.label_masks = label_masks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.inputs.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        item['label_masks'] = self.label_masks[idx]\n",
    "        return item\n",
    "\n",
    "dataset = MultiTaskDataset(inputs, labels, label_masks)\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 128\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model.to(device)\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# 自定义损失函数，忽略掩码位置的计算\n",
    "def masked_loss(outputs, labels, label_masks):\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "    # 忽略-1填充的标签\n",
    "    active_loss = labels != -1\n",
    "    active_labels = labels[active_loss]\n",
    "    active_outputs = outputs[active_loss]\n",
    "    losses = loss_fn(active_outputs, active_labels)\n",
    "    masked_losses = losses * label_masks[active_loss]\n",
    "    return masked_losses.sum() / label_masks.sum()\n",
    "\n",
    "# 训练模型\n",
    "model.train()\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        label_masks = batch['label_masks']\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        loss = masked_loss(outputs, labels, label_masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss}\")\n",
    "\n",
    "\n",
    "# 保存模型\n",
    "model.save_pretrained(\"saved_model\")\n",
    "tokenizer.save_pretrained(\"saved_model\")\n",
    "\n",
    "    \n",
    "\n",
    "# 获取化学分子的表示和预测值\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.hidden_states[-1][:, 0, :]  # 获取 [CLS] token 的表示\n",
    "    predictions = torch.sigmoid(outputs.logits)  # 获取每个标签的预测概率\n",
    "\n",
    "# 拼接表示和预测值\n",
    "enhanced_embeddings = torch.cat((embeddings, predictions), dim=1)\n",
    "\n",
    "print(\"增强的嵌入表示和预测值获取成功。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5443c3af-82bd-4d72-9421-bf4a95aeb485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6f77d27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词器和模型加载成功。\n",
      "增强的嵌入表示： tensor([[-0.7026,  0.9633,  0.0563,  0.8383,  0.0945, -0.2631,  1.1302,  0.1299,\n",
      "          0.0561, -0.9723, -0.9828, -1.4893,  0.6511,  0.7763,  0.8283,  0.5899,\n",
      "         -1.4367, -1.6326,  0.1385,  0.2468,  1.7175,  1.1200,  1.0265,  0.1929,\n",
      "         -0.9465, -0.1148, -0.5993,  0.8092, -0.2948,  0.1433, -0.8075,  2.2088,\n",
      "         -0.4450,  0.6579,  2.2802, -0.6125,  0.7587,  0.1949, -0.2592, -0.1243,\n",
      "          0.0946,  0.9520, -0.6827,  0.3262,  0.2294, -0.3793,  0.0517,  0.0342,\n",
      "         -0.8513,  0.3363, -0.1308,  0.9110, -0.4086, -0.2750,  0.3098,  0.0882,\n",
      "          0.3114, -0.8604,  0.3711, -0.9468, -1.1517, -0.2328,  0.1529, -0.8666,\n",
      "         -0.8668, -0.2006, -0.3323,  1.1816,  0.9937, -0.0444,  1.0080,  0.7024,\n",
      "          0.7091, -0.1850, -0.9922, -0.4285,  0.3310, -1.4804,  1.1037,  0.3529,\n",
      "         -0.4342, -0.6479,  0.3664,  0.1896,  1.0506, -0.7257,  0.7219, -0.6116,\n",
      "         -1.0369,  0.5260,  0.1250, -0.7709, -1.1416,  1.3115, -0.4780, -0.0785,\n",
      "         -0.0600,  0.8203, -0.2798, -0.5169,  1.0312, -0.1702,  1.8506,  0.4991,\n",
      "         -0.6513, -1.0506,  1.6591, -0.9087,  0.0533,  0.7234,  0.8719, -0.0899,\n",
      "          1.1459,  0.1069, -0.1361, -1.8972, -0.0476, -0.6792,  1.0743,  0.0117,\n",
      "         -0.6252, -0.2220,  0.1469,  0.4995,  0.1408, -0.5744, -0.1350,  1.6157,\n",
      "         -1.6528,  0.5186,  0.8612,  0.3959, -1.1878,  0.9356,  0.1085, -0.4227,\n",
      "         -0.0668,  0.4746, -0.2206, -0.6264, -0.2487,  1.2003,  0.4418, -0.1693,\n",
      "          0.7230, -1.9204, -0.2071, -1.5697,  0.8181, -0.3519,  0.1848,  0.3972,\n",
      "          0.8818,  1.4764, -0.0173,  0.2794, -0.3795, -1.3455,  0.4539,  0.9341,\n",
      "         -0.1295, -0.2491, -0.0789, -0.9053, -0.2943,  1.4010, -0.3394,  0.3112,\n",
      "          0.3527, -0.3158,  0.8872, -0.2977, -0.1438, -1.1990,  0.2476, -0.0420,\n",
      "          2.3302, -0.2243,  0.3657,  0.4159,  0.6508, -0.1151, -0.1302,  0.2762,\n",
      "          0.1141, -1.0023, -1.6881,  0.3438,  0.5315, -0.2320,  0.9539,  0.7880,\n",
      "         -0.3184,  0.5795, -0.6694,  0.1951, -1.1228,  0.4855,  0.4729,  0.2604,\n",
      "          0.1095, -0.6996, -1.0560,  0.9696, -1.0949, -0.9563, -0.0245, -1.2037,\n",
      "         -0.9733, -0.6838, -0.9286, -0.2557,  1.3066,  0.0899, -1.3005, -0.5520,\n",
      "         -1.0594,  0.6145,  0.2458, -0.3820, -0.2175,  0.6635, -0.6087,  1.0650,\n",
      "         -0.5067, -0.5100, -0.3096, -0.3004, -0.4966,  0.9066,  1.2282,  0.3367,\n",
      "          1.7609,  0.3913, -0.9551,  0.3017,  0.1098, -0.5216, -0.0632, -1.1508,\n",
      "          0.0173, -0.0831,  0.9408, -1.0625,  0.2271,  0.3443,  0.1694,  0.5315,\n",
      "         -0.4359,  0.1175, -2.9491, -0.7084,  0.2231,  0.0569,  0.9004,  1.3691,\n",
      "         -1.7536, -0.7007,  0.4803, -0.8093,  0.9315, -0.9184, -0.2475,  0.2899,\n",
      "          1.6211, -0.8080, -0.1941, -0.4423,  0.7425, -1.2681, -0.7101, -0.0637,\n",
      "          0.9872,  0.9306,  0.1516, -0.3197,  0.5426, -0.1908,  0.5804, -0.5727,\n",
      "          0.2934,  0.9891, -0.2915,  0.4039, -0.9159, -0.5631, -0.6078, -0.0055,\n",
      "          1.0770, -0.8465, -0.2775, -1.1255, -1.0515, -0.4543, -1.5238, -0.8078,\n",
      "         -0.0915, -0.1300,  0.6597, -0.6445, -0.6626,  0.1486, -0.5454, -2.1247,\n",
      "          0.2287, -0.0277,  2.0546,  0.2467, -0.0426,  0.5069,  0.4311, -0.5335,\n",
      "         -0.1163, -0.4506,  1.3048,  0.5639,  0.2946, -0.0562,  1.2867,  1.0810,\n",
      "          0.3740, -0.0798, -1.9417, -0.9403,  0.2722,  0.1840, -1.2512, -0.2553,\n",
      "         -0.4377, -0.3720, -0.2144,  2.0389, -0.8242, -0.6703,  1.1194, -0.3869,\n",
      "          1.1845,  0.6764, -1.2639,  0.8105, -1.3705,  0.3592, -1.2869,  1.4587,\n",
      "          0.4366,  0.7324, -0.5174,  0.5903, -0.0777, -0.2760, -0.0976,  0.9054,\n",
      "         -1.3124, -0.4905,  0.4783,  0.8934,  1.2389,  0.3710, -0.7104,  1.0486,\n",
      "          0.5327,  1.0966, -0.6134,  0.7708, -1.3613, -0.0646, -0.7644,  0.5505,\n",
      "          0.9630, -1.2335,  1.1260, -0.2362,  0.5770, -2.8206, -0.2645, -0.7168,\n",
      "          0.4625,  1.9211,  1.4031,  0.9198, -0.8846,  0.3385,  0.2964, -0.8223,\n",
      "          0.1583,  0.0777,  0.2513,  0.3160,  0.2542,  0.1903,  0.2853,  0.2847,\n",
      "          0.3274,  0.2888,  0.0949,  0.0657,  0.1678,  0.0537,  0.0617,  0.0806,\n",
      "          0.0585,  0.0752,  0.0643,  0.0818,  0.0953,  0.0961,  0.1128,  0.1779,\n",
      "          0.1346,  0.1321,  0.0678,  0.0861,  0.1085,  0.1871,  0.1812,  0.1066,\n",
      "          0.0805,  0.1307,  0.1754,  0.1971,  0.0608,  0.0601]])\n",
      "表示的维度： torch.Size([1, 422])\n"
     ]
    }
   ],
   "source": [
    "#以一个化学物的smiles式子为例，得到增强的表示\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "import torch\n",
    "\n",
    "# 指定本地模型路径\n",
    "model_path = \"saved_model\"\n",
    "\n",
    "# 从本地路径加载分词器和模型配置\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "config.num_labels = 38  # 手动设置标签数量为 21\n",
    "config.output_hidden_states = True\n",
    "\n",
    "# 从本地路径加载具有更新配置的模型\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, config=config)\n",
    "\n",
    "print(\"分词器和模型加载成功。\")\n",
    "\n",
    "# 输入化学物的SMILES序列\n",
    "smiles_sequence = \"COCOC\"\n",
    "\n",
    "# 标记化输入数据\n",
    "inputs = tokenizer(smiles_sequence, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# 获取模型的输出\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.hidden_states[-1][:, 0, :]  # 获取 [CLS] token 的表示\n",
    "    predictions = torch.sigmoid(outputs.logits)  # 获取每个标签的预测概率\n",
    "\n",
    "# 拼接表示和预测值\n",
    "enhanced_embeddings = torch.cat((embeddings, predictions), dim=1)\n",
    "\n",
    "# 输出结果\n",
    "print(\"增强的嵌入表示：\", enhanced_embeddings)\n",
    "print(\"表示的维度：\", enhanced_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2305c113",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Loss: 0.7156856656074524, Test Accuracy: 0.3972\n",
      "Epoch 2/500, Loss: 0.711452305316925, Test Accuracy: 0.4113\n",
      "Epoch 3/500, Loss: 0.707378089427948, Test Accuracy: 0.4043\n",
      "Epoch 4/500, Loss: 0.7065422534942627, Test Accuracy: 0.4184\n",
      "Epoch 5/500, Loss: 0.695981502532959, Test Accuracy: 0.4113\n",
      "Epoch 6/500, Loss: 0.6993508338928223, Test Accuracy: 0.3972\n",
      "Epoch 7/500, Loss: 0.6988183856010437, Test Accuracy: 0.4184\n",
      "Epoch 8/500, Loss: 0.6943937540054321, Test Accuracy: 0.4255\n",
      "Epoch 9/500, Loss: 0.6878546476364136, Test Accuracy: 0.4468\n",
      "Epoch 10/500, Loss: 0.6860702037811279, Test Accuracy: 0.4539\n",
      "Epoch 11/500, Loss: 0.6868847608566284, Test Accuracy: 0.4539\n",
      "Epoch 12/500, Loss: 0.6804890036582947, Test Accuracy: 0.4681\n",
      "Epoch 13/500, Loss: 0.6769543886184692, Test Accuracy: 0.4752\n",
      "Epoch 14/500, Loss: 0.675348162651062, Test Accuracy: 0.4894\n",
      "Epoch 15/500, Loss: 0.6753955483436584, Test Accuracy: 0.4752\n",
      "Epoch 16/500, Loss: 0.6741563081741333, Test Accuracy: 0.4752\n",
      "Epoch 17/500, Loss: 0.6663344502449036, Test Accuracy: 0.4823\n",
      "Epoch 18/500, Loss: 0.6658493280410767, Test Accuracy: 0.4823\n",
      "Epoch 19/500, Loss: 0.6685104370117188, Test Accuracy: 0.4894\n",
      "Epoch 20/500, Loss: 0.6630599498748779, Test Accuracy: 0.4823\n",
      "Epoch 21/500, Loss: 0.6596538424491882, Test Accuracy: 0.5177\n",
      "Epoch 22/500, Loss: 0.6597391963005066, Test Accuracy: 0.5248\n",
      "Epoch 23/500, Loss: 0.6531655192375183, Test Accuracy: 0.5319\n",
      "Epoch 24/500, Loss: 0.6577602624893188, Test Accuracy: 0.5319\n",
      "Epoch 25/500, Loss: 0.6477885842323303, Test Accuracy: 0.5461\n",
      "Epoch 26/500, Loss: 0.6507261991500854, Test Accuracy: 0.5532\n",
      "Epoch 27/500, Loss: 0.6459874510765076, Test Accuracy: 0.5461\n",
      "Epoch 28/500, Loss: 0.6446507573127747, Test Accuracy: 0.5461\n",
      "Epoch 29/500, Loss: 0.6454777121543884, Test Accuracy: 0.5461\n",
      "Epoch 30/500, Loss: 0.6395540237426758, Test Accuracy: 0.5532\n",
      "Epoch 31/500, Loss: 0.6383050084114075, Test Accuracy: 0.5532\n",
      "Epoch 32/500, Loss: 0.6414669156074524, Test Accuracy: 0.5532\n",
      "Epoch 33/500, Loss: 0.6321637034416199, Test Accuracy: 0.5603\n",
      "Epoch 34/500, Loss: 0.6289942860603333, Test Accuracy: 0.5532\n",
      "Epoch 35/500, Loss: 0.6338778734207153, Test Accuracy: 0.5532\n",
      "Epoch 36/500, Loss: 0.6232490539550781, Test Accuracy: 0.5532\n",
      "Epoch 37/500, Loss: 0.6304439306259155, Test Accuracy: 0.5532\n",
      "Epoch 38/500, Loss: 0.6282922029495239, Test Accuracy: 0.5603\n",
      "Epoch 39/500, Loss: 0.6248270869255066, Test Accuracy: 0.5603\n",
      "Epoch 40/500, Loss: 0.6207883954048157, Test Accuracy: 0.5603\n",
      "Epoch 41/500, Loss: 0.6216866374015808, Test Accuracy: 0.5674\n",
      "Epoch 42/500, Loss: 0.6149311661720276, Test Accuracy: 0.5674\n",
      "Epoch 43/500, Loss: 0.6144851446151733, Test Accuracy: 0.5674\n",
      "Epoch 44/500, Loss: 0.6059689521789551, Test Accuracy: 0.5674\n",
      "Epoch 45/500, Loss: 0.606434166431427, Test Accuracy: 0.5603\n",
      "Epoch 46/500, Loss: 0.607779860496521, Test Accuracy: 0.5674\n",
      "Epoch 47/500, Loss: 0.5964788198471069, Test Accuracy: 0.5816\n",
      "Epoch 48/500, Loss: 0.5978220105171204, Test Accuracy: 0.5887\n",
      "Epoch 49/500, Loss: 0.6008720993995667, Test Accuracy: 0.5887\n",
      "Epoch 50/500, Loss: 0.597648024559021, Test Accuracy: 0.5816\n",
      "Epoch 51/500, Loss: 0.5982659459114075, Test Accuracy: 0.5887\n",
      "Epoch 52/500, Loss: 0.5830731987953186, Test Accuracy: 0.5816\n",
      "Epoch 53/500, Loss: 0.5777508616447449, Test Accuracy: 0.5816\n",
      "Epoch 54/500, Loss: 0.5874428153038025, Test Accuracy: 0.5674\n",
      "Epoch 55/500, Loss: 0.5731971859931946, Test Accuracy: 0.5745\n",
      "Epoch 56/500, Loss: 0.5747705698013306, Test Accuracy: 0.5816\n",
      "Epoch 57/500, Loss: 0.5769379138946533, Test Accuracy: 0.5745\n",
      "Epoch 58/500, Loss: 0.5696092247962952, Test Accuracy: 0.5745\n",
      "Epoch 59/500, Loss: 0.5723453164100647, Test Accuracy: 0.5816\n",
      "Epoch 60/500, Loss: 0.5613535046577454, Test Accuracy: 0.5887\n",
      "Epoch 61/500, Loss: 0.568395733833313, Test Accuracy: 0.5887\n",
      "Epoch 62/500, Loss: 0.5584086179733276, Test Accuracy: 0.5887\n",
      "Epoch 63/500, Loss: 0.5573562383651733, Test Accuracy: 0.5887\n",
      "Epoch 64/500, Loss: 0.5513859987258911, Test Accuracy: 0.5887\n",
      "Epoch 65/500, Loss: 0.5472265481948853, Test Accuracy: 0.5887\n",
      "Epoch 66/500, Loss: 0.5483505725860596, Test Accuracy: 0.5887\n",
      "Epoch 67/500, Loss: 0.5446922779083252, Test Accuracy: 0.5957\n",
      "Epoch 68/500, Loss: 0.5455715656280518, Test Accuracy: 0.5957\n",
      "Epoch 69/500, Loss: 0.5385690331459045, Test Accuracy: 0.6028\n",
      "Epoch 70/500, Loss: 0.5487936735153198, Test Accuracy: 0.6028\n",
      "Epoch 71/500, Loss: 0.5361846685409546, Test Accuracy: 0.6099\n",
      "Epoch 72/500, Loss: 0.5405241250991821, Test Accuracy: 0.6170\n",
      "Epoch 73/500, Loss: 0.5286566019058228, Test Accuracy: 0.6170\n",
      "Epoch 74/500, Loss: 0.5256893038749695, Test Accuracy: 0.6170\n",
      "Epoch 75/500, Loss: 0.5272760391235352, Test Accuracy: 0.6170\n",
      "Epoch 76/500, Loss: 0.5145231485366821, Test Accuracy: 0.6241\n",
      "Epoch 77/500, Loss: 0.5170162320137024, Test Accuracy: 0.6241\n",
      "Epoch 78/500, Loss: 0.5083480477333069, Test Accuracy: 0.6241\n",
      "Epoch 79/500, Loss: 0.5003117322921753, Test Accuracy: 0.6241\n",
      "Epoch 80/500, Loss: 0.5018174052238464, Test Accuracy: 0.6312\n",
      "Epoch 81/500, Loss: 0.49805518984794617, Test Accuracy: 0.6383\n",
      "Epoch 82/500, Loss: 0.513419508934021, Test Accuracy: 0.6383\n",
      "Epoch 83/500, Loss: 0.5009011030197144, Test Accuracy: 0.6383\n",
      "Epoch 84/500, Loss: 0.48287439346313477, Test Accuracy: 0.6383\n",
      "Epoch 85/500, Loss: 0.49175766110420227, Test Accuracy: 0.6383\n",
      "Epoch 86/500, Loss: 0.4766274392604828, Test Accuracy: 0.6383\n",
      "Epoch 87/500, Loss: 0.4801311790943146, Test Accuracy: 0.6383\n",
      "Epoch 88/500, Loss: 0.48066845536231995, Test Accuracy: 0.6383\n",
      "Epoch 89/500, Loss: 0.4824853539466858, Test Accuracy: 0.6383\n",
      "Epoch 90/500, Loss: 0.47942519187927246, Test Accuracy: 0.6312\n",
      "Epoch 91/500, Loss: 0.47793737053871155, Test Accuracy: 0.6312\n",
      "Epoch 92/500, Loss: 0.4584742486476898, Test Accuracy: 0.6312\n",
      "Epoch 93/500, Loss: 0.46694216132164, Test Accuracy: 0.6312\n",
      "Epoch 94/500, Loss: 0.4705507755279541, Test Accuracy: 0.6383\n",
      "Epoch 95/500, Loss: 0.4533613920211792, Test Accuracy: 0.6454\n",
      "Epoch 96/500, Loss: 0.4583892226219177, Test Accuracy: 0.6454\n",
      "Epoch 97/500, Loss: 0.4395187199115753, Test Accuracy: 0.6525\n",
      "Epoch 98/500, Loss: 0.4433487057685852, Test Accuracy: 0.6596\n",
      "Epoch 99/500, Loss: 0.4415726363658905, Test Accuracy: 0.6525\n",
      "Epoch 100/500, Loss: 0.43464913964271545, Test Accuracy: 0.6525\n",
      "Epoch 101/500, Loss: 0.42933157086372375, Test Accuracy: 0.6525\n",
      "Epoch 102/500, Loss: 0.4338444769382477, Test Accuracy: 0.6596\n",
      "Epoch 103/500, Loss: 0.4240753948688507, Test Accuracy: 0.6596\n",
      "Epoch 104/500, Loss: 0.4374592900276184, Test Accuracy: 0.6596\n",
      "Epoch 105/500, Loss: 0.4163742661476135, Test Accuracy: 0.6667\n",
      "Epoch 106/500, Loss: 0.4132586121559143, Test Accuracy: 0.6667\n",
      "Epoch 107/500, Loss: 0.4246085286140442, Test Accuracy: 0.6667\n",
      "Epoch 108/500, Loss: 0.42787066102027893, Test Accuracy: 0.6596\n",
      "Epoch 109/500, Loss: 0.41219696402549744, Test Accuracy: 0.6809\n",
      "Epoch 110/500, Loss: 0.4099041521549225, Test Accuracy: 0.6809\n",
      "Epoch 111/500, Loss: 0.3849245607852936, Test Accuracy: 0.6809\n",
      "Epoch 112/500, Loss: 0.39478781819343567, Test Accuracy: 0.6809\n",
      "Epoch 113/500, Loss: 0.39388972520828247, Test Accuracy: 0.6809\n",
      "Epoch 114/500, Loss: 0.3879455327987671, Test Accuracy: 0.6738\n",
      "Epoch 115/500, Loss: 0.3891187310218811, Test Accuracy: 0.6738\n",
      "Epoch 116/500, Loss: 0.38233187794685364, Test Accuracy: 0.6809\n",
      "Epoch 117/500, Loss: 0.3852554261684418, Test Accuracy: 0.6738\n",
      "Epoch 118/500, Loss: 0.3707336485385895, Test Accuracy: 0.6809\n",
      "Epoch 119/500, Loss: 0.37810125946998596, Test Accuracy: 0.6879\n",
      "Epoch 120/500, Loss: 0.3814713954925537, Test Accuracy: 0.6879\n",
      "Epoch 121/500, Loss: 0.37276962399482727, Test Accuracy: 0.6879\n",
      "Epoch 122/500, Loss: 0.3579804599285126, Test Accuracy: 0.6950\n",
      "Epoch 123/500, Loss: 0.35133862495422363, Test Accuracy: 0.6809\n",
      "Epoch 124/500, Loss: 0.35927116870880127, Test Accuracy: 0.6809\n",
      "Epoch 125/500, Loss: 0.3476867079734802, Test Accuracy: 0.6809\n",
      "Epoch 126/500, Loss: 0.35302746295928955, Test Accuracy: 0.6809\n",
      "Epoch 127/500, Loss: 0.3475438356399536, Test Accuracy: 0.6809\n",
      "Epoch 128/500, Loss: 0.34355825185775757, Test Accuracy: 0.6879\n",
      "Epoch 129/500, Loss: 0.3313646912574768, Test Accuracy: 0.6950\n",
      "Epoch 130/500, Loss: 0.32659947872161865, Test Accuracy: 0.6879\n",
      "Epoch 131/500, Loss: 0.31840401887893677, Test Accuracy: 0.6879\n",
      "Epoch 132/500, Loss: 0.3262164294719696, Test Accuracy: 0.6809\n",
      "Epoch 133/500, Loss: 0.30369722843170166, Test Accuracy: 0.6809\n",
      "Epoch 134/500, Loss: 0.32428330183029175, Test Accuracy: 0.6809\n",
      "Epoch 135/500, Loss: 0.3103499114513397, Test Accuracy: 0.6738\n",
      "Epoch 136/500, Loss: 0.3083178997039795, Test Accuracy: 0.6738\n",
      "Epoch 137/500, Loss: 0.3072429597377777, Test Accuracy: 0.6667\n",
      "Epoch 138/500, Loss: 0.3188856542110443, Test Accuracy: 0.6667\n",
      "Epoch 139/500, Loss: 0.310656875371933, Test Accuracy: 0.6667\n",
      "Epoch 140/500, Loss: 0.2971859276294708, Test Accuracy: 0.6738\n",
      "Epoch 141/500, Loss: 0.29728102684020996, Test Accuracy: 0.6738\n",
      "Epoch 142/500, Loss: 0.29200708866119385, Test Accuracy: 0.6738\n",
      "Epoch 143/500, Loss: 0.2884555160999298, Test Accuracy: 0.6738\n",
      "Epoch 144/500, Loss: 0.2866275906562805, Test Accuracy: 0.6738\n",
      "Epoch 145/500, Loss: 0.29794687032699585, Test Accuracy: 0.6738\n",
      "Epoch 146/500, Loss: 0.2822498679161072, Test Accuracy: 0.6738\n",
      "Epoch 147/500, Loss: 0.2651083767414093, Test Accuracy: 0.6738\n",
      "Epoch 148/500, Loss: 0.269148051738739, Test Accuracy: 0.6738\n",
      "Epoch 149/500, Loss: 0.26799121499061584, Test Accuracy: 0.6667\n",
      "Epoch 150/500, Loss: 0.25486239790916443, Test Accuracy: 0.6667\n",
      "Epoch 151/500, Loss: 0.24957303702831268, Test Accuracy: 0.6667\n",
      "Epoch 152/500, Loss: 0.257556289434433, Test Accuracy: 0.6667\n",
      "Epoch 153/500, Loss: 0.24042822420597076, Test Accuracy: 0.6667\n",
      "Epoch 154/500, Loss: 0.23642553389072418, Test Accuracy: 0.6667\n",
      "Epoch 155/500, Loss: 0.2423468381166458, Test Accuracy: 0.6667\n",
      "Epoch 156/500, Loss: 0.23720909655094147, Test Accuracy: 0.6667\n",
      "Epoch 157/500, Loss: 0.23337893187999725, Test Accuracy: 0.6667\n",
      "Epoch 158/500, Loss: 0.22966594994068146, Test Accuracy: 0.6738\n",
      "Epoch 159/500, Loss: 0.20412608981132507, Test Accuracy: 0.6738\n",
      "Epoch 160/500, Loss: 0.23296956717967987, Test Accuracy: 0.6738\n",
      "Epoch 161/500, Loss: 0.22914324700832367, Test Accuracy: 0.6738\n",
      "Epoch 162/500, Loss: 0.22028475999832153, Test Accuracy: 0.6738\n",
      "Epoch 163/500, Loss: 0.21658428013324738, Test Accuracy: 0.6809\n",
      "Epoch 164/500, Loss: 0.22545461356639862, Test Accuracy: 0.6950\n",
      "Epoch 165/500, Loss: 0.2185312807559967, Test Accuracy: 0.6879\n",
      "Epoch 166/500, Loss: 0.2017713338136673, Test Accuracy: 0.6879\n",
      "Epoch 167/500, Loss: 0.20710112154483795, Test Accuracy: 0.7163\n",
      "Epoch 168/500, Loss: 0.19517672061920166, Test Accuracy: 0.7234\n",
      "Epoch 169/500, Loss: 0.19335664808750153, Test Accuracy: 0.7305\n",
      "Epoch 170/500, Loss: 0.19850657880306244, Test Accuracy: 0.7305\n",
      "Epoch 171/500, Loss: 0.2012125551700592, Test Accuracy: 0.7305\n",
      "Epoch 172/500, Loss: 0.18480491638183594, Test Accuracy: 0.7163\n",
      "Epoch 173/500, Loss: 0.19367460906505585, Test Accuracy: 0.7163\n",
      "Epoch 174/500, Loss: 0.1735777109861374, Test Accuracy: 0.7163\n",
      "Epoch 175/500, Loss: 0.18356429040431976, Test Accuracy: 0.7234\n",
      "Epoch 176/500, Loss: 0.17455460131168365, Test Accuracy: 0.7234\n",
      "Epoch 177/500, Loss: 0.17557741701602936, Test Accuracy: 0.7163\n",
      "Epoch 178/500, Loss: 0.18871797621250153, Test Accuracy: 0.7092\n",
      "Epoch 179/500, Loss: 0.17776061594486237, Test Accuracy: 0.7092\n",
      "Epoch 180/500, Loss: 0.16431736946105957, Test Accuracy: 0.7092\n",
      "Epoch 181/500, Loss: 0.1566784381866455, Test Accuracy: 0.7092\n",
      "Epoch 182/500, Loss: 0.1712709367275238, Test Accuracy: 0.7092\n",
      "Epoch 183/500, Loss: 0.1540840119123459, Test Accuracy: 0.7092\n",
      "Epoch 184/500, Loss: 0.16360633075237274, Test Accuracy: 0.7092\n",
      "Epoch 185/500, Loss: 0.1536254584789276, Test Accuracy: 0.7092\n",
      "Epoch 186/500, Loss: 0.15570998191833496, Test Accuracy: 0.7163\n",
      "Epoch 187/500, Loss: 0.16395561397075653, Test Accuracy: 0.7163\n",
      "Epoch 188/500, Loss: 0.13930350542068481, Test Accuracy: 0.7163\n",
      "Epoch 189/500, Loss: 0.14660373330116272, Test Accuracy: 0.7234\n",
      "Epoch 190/500, Loss: 0.14981482923030853, Test Accuracy: 0.7234\n",
      "Epoch 191/500, Loss: 0.14044393599033356, Test Accuracy: 0.7234\n",
      "Epoch 192/500, Loss: 0.144348606467247, Test Accuracy: 0.7234\n",
      "Epoch 193/500, Loss: 0.15147005021572113, Test Accuracy: 0.7305\n",
      "Epoch 194/500, Loss: 0.14180326461791992, Test Accuracy: 0.7234\n",
      "Epoch 195/500, Loss: 0.1303883194923401, Test Accuracy: 0.7234\n",
      "Epoch 196/500, Loss: 0.1131494864821434, Test Accuracy: 0.7234\n",
      "Epoch 197/500, Loss: 0.12151657789945602, Test Accuracy: 0.7234\n",
      "Epoch 198/500, Loss: 0.12419626116752625, Test Accuracy: 0.7234\n",
      "Epoch 199/500, Loss: 0.1129472553730011, Test Accuracy: 0.7305\n",
      "Epoch 200/500, Loss: 0.11967117339372635, Test Accuracy: 0.7305\n",
      "Epoch 201/500, Loss: 0.12827779352664948, Test Accuracy: 0.7234\n",
      "Epoch 202/500, Loss: 0.13618431985378265, Test Accuracy: 0.7163\n",
      "Epoch 203/500, Loss: 0.11937471479177475, Test Accuracy: 0.7163\n",
      "Epoch 204/500, Loss: 0.12042724341154099, Test Accuracy: 0.7163\n",
      "Epoch 205/500, Loss: 0.11891063302755356, Test Accuracy: 0.7163\n",
      "Epoch 206/500, Loss: 0.11490440368652344, Test Accuracy: 0.7234\n",
      "Epoch 207/500, Loss: 0.12136491388082504, Test Accuracy: 0.7305\n",
      "Epoch 208/500, Loss: 0.10563583672046661, Test Accuracy: 0.7376\n",
      "Epoch 209/500, Loss: 0.1045667976140976, Test Accuracy: 0.7376\n",
      "Epoch 210/500, Loss: 0.11518020182847977, Test Accuracy: 0.7376\n",
      "Epoch 211/500, Loss: 0.11838290840387344, Test Accuracy: 0.7376\n",
      "Epoch 212/500, Loss: 0.09685107320547104, Test Accuracy: 0.7447\n",
      "Epoch 213/500, Loss: 0.11660269647836685, Test Accuracy: 0.7447\n",
      "Epoch 214/500, Loss: 0.09993111342191696, Test Accuracy: 0.7447\n",
      "Epoch 215/500, Loss: 0.10637351870536804, Test Accuracy: 0.7518\n",
      "Epoch 216/500, Loss: 0.08731691539287567, Test Accuracy: 0.7518\n",
      "Epoch 217/500, Loss: 0.09740506112575531, Test Accuracy: 0.7376\n",
      "Epoch 218/500, Loss: 0.10214575380086899, Test Accuracy: 0.7376\n",
      "Epoch 219/500, Loss: 0.0938107967376709, Test Accuracy: 0.7376\n",
      "Epoch 220/500, Loss: 0.10430635511875153, Test Accuracy: 0.7376\n",
      "Epoch 221/500, Loss: 0.08507539331912994, Test Accuracy: 0.7376\n",
      "Epoch 222/500, Loss: 0.08982864022254944, Test Accuracy: 0.7376\n",
      "Epoch 223/500, Loss: 0.06999840587377548, Test Accuracy: 0.7447\n",
      "Epoch 224/500, Loss: 0.09794191271066666, Test Accuracy: 0.7447\n",
      "Epoch 225/500, Loss: 0.0892505869269371, Test Accuracy: 0.7447\n",
      "Epoch 226/500, Loss: 0.09092605859041214, Test Accuracy: 0.7447\n",
      "Epoch 227/500, Loss: 0.08902522921562195, Test Accuracy: 0.7447\n",
      "Epoch 228/500, Loss: 0.08738234639167786, Test Accuracy: 0.7518\n",
      "Epoch 229/500, Loss: 0.0753321498632431, Test Accuracy: 0.7518\n",
      "Epoch 230/500, Loss: 0.09783009439706802, Test Accuracy: 0.7518\n",
      "Epoch 231/500, Loss: 0.08154993504285812, Test Accuracy: 0.7447\n",
      "Epoch 232/500, Loss: 0.08769691735506058, Test Accuracy: 0.7447\n",
      "Epoch 233/500, Loss: 0.08074739575386047, Test Accuracy: 0.7376\n",
      "Epoch 234/500, Loss: 0.07300286740064621, Test Accuracy: 0.7305\n",
      "Epoch 235/500, Loss: 0.08408615738153458, Test Accuracy: 0.7305\n",
      "Epoch 236/500, Loss: 0.07257655262947083, Test Accuracy: 0.7305\n",
      "Epoch 237/500, Loss: 0.07614969462156296, Test Accuracy: 0.7305\n",
      "Epoch 238/500, Loss: 0.08093278855085373, Test Accuracy: 0.7305\n",
      "Epoch 239/500, Loss: 0.06662885844707489, Test Accuracy: 0.7376\n",
      "Epoch 240/500, Loss: 0.08344423770904541, Test Accuracy: 0.7447\n",
      "Epoch 241/500, Loss: 0.07978373765945435, Test Accuracy: 0.7376\n",
      "Epoch 242/500, Loss: 0.06876213848590851, Test Accuracy: 0.7376\n",
      "Epoch 243/500, Loss: 0.06877786666154861, Test Accuracy: 0.7376\n",
      "Epoch 244/500, Loss: 0.08066205680370331, Test Accuracy: 0.7305\n",
      "Epoch 245/500, Loss: 0.06533505022525787, Test Accuracy: 0.7305\n",
      "Epoch 246/500, Loss: 0.08452045172452927, Test Accuracy: 0.7305\n",
      "Epoch 247/500, Loss: 0.07672847807407379, Test Accuracy: 0.7305\n",
      "Epoch 248/500, Loss: 0.06369026750326157, Test Accuracy: 0.7305\n",
      "Epoch 249/500, Loss: 0.048989906907081604, Test Accuracy: 0.7305\n",
      "Epoch 250/500, Loss: 0.07068872451782227, Test Accuracy: 0.7305\n",
      "Epoch 251/500, Loss: 0.06589779257774353, Test Accuracy: 0.7305\n",
      "Epoch 252/500, Loss: 0.06459614634513855, Test Accuracy: 0.7305\n",
      "Epoch 253/500, Loss: 0.0708431601524353, Test Accuracy: 0.7305\n",
      "Epoch 254/500, Loss: 0.05514185503125191, Test Accuracy: 0.7305\n",
      "Epoch 255/500, Loss: 0.05903884395956993, Test Accuracy: 0.7305\n",
      "Epoch 256/500, Loss: 0.070401132106781, Test Accuracy: 0.7305\n",
      "Epoch 257/500, Loss: 0.059340544044971466, Test Accuracy: 0.7305\n",
      "Epoch 258/500, Loss: 0.049390822649002075, Test Accuracy: 0.7305\n",
      "Epoch 259/500, Loss: 0.057497117668390274, Test Accuracy: 0.7305\n",
      "Epoch 260/500, Loss: 0.06638000905513763, Test Accuracy: 0.7305\n",
      "Epoch 261/500, Loss: 0.056362491101026535, Test Accuracy: 0.7305\n",
      "Epoch 262/500, Loss: 0.05448869243264198, Test Accuracy: 0.7305\n",
      "Epoch 263/500, Loss: 0.04709860682487488, Test Accuracy: 0.7305\n",
      "Epoch 264/500, Loss: 0.061096083372831345, Test Accuracy: 0.7305\n",
      "Epoch 265/500, Loss: 0.05283290892839432, Test Accuracy: 0.7305\n",
      "Epoch 266/500, Loss: 0.06493580341339111, Test Accuracy: 0.7305\n",
      "Epoch 267/500, Loss: 0.061547067016363144, Test Accuracy: 0.7305\n",
      "Epoch 268/500, Loss: 0.03948618844151497, Test Accuracy: 0.7305\n",
      "Epoch 269/500, Loss: 0.06346950680017471, Test Accuracy: 0.7305\n",
      "Epoch 270/500, Loss: 0.05217462405562401, Test Accuracy: 0.7376\n",
      "Epoch 271/500, Loss: 0.043358296155929565, Test Accuracy: 0.7376\n",
      "Epoch 272/500, Loss: 0.04869799688458443, Test Accuracy: 0.7305\n",
      "Epoch 273/500, Loss: 0.05787290260195732, Test Accuracy: 0.7305\n",
      "Epoch 274/500, Loss: 0.06417406350374222, Test Accuracy: 0.7305\n",
      "Epoch 275/500, Loss: 0.03790576756000519, Test Accuracy: 0.7305\n",
      "Epoch 276/500, Loss: 0.05857544392347336, Test Accuracy: 0.7305\n",
      "Epoch 277/500, Loss: 0.04150586947798729, Test Accuracy: 0.7305\n",
      "Epoch 278/500, Loss: 0.056061871349811554, Test Accuracy: 0.7305\n",
      "Epoch 279/500, Loss: 0.04451599344611168, Test Accuracy: 0.7305\n",
      "Epoch 280/500, Loss: 0.04932104051113129, Test Accuracy: 0.7305\n",
      "Epoch 281/500, Loss: 0.034482307732105255, Test Accuracy: 0.7305\n",
      "Epoch 282/500, Loss: 0.04100389778614044, Test Accuracy: 0.7305\n",
      "Epoch 283/500, Loss: 0.054037947207689285, Test Accuracy: 0.7305\n",
      "Epoch 284/500, Loss: 0.059044502675533295, Test Accuracy: 0.7305\n",
      "Epoch 285/500, Loss: 0.053029585629701614, Test Accuracy: 0.7305\n",
      "Epoch 286/500, Loss: 0.03290431201457977, Test Accuracy: 0.7305\n",
      "Epoch 287/500, Loss: 0.04560846835374832, Test Accuracy: 0.7305\n",
      "Epoch 288/500, Loss: 0.02856515161693096, Test Accuracy: 0.7305\n",
      "Epoch 289/500, Loss: 0.05343609303236008, Test Accuracy: 0.7376\n",
      "Epoch 290/500, Loss: 0.05250206217169762, Test Accuracy: 0.7305\n",
      "Epoch 291/500, Loss: 0.036216966807842255, Test Accuracy: 0.7376\n",
      "Epoch 292/500, Loss: 0.04028595983982086, Test Accuracy: 0.7376\n",
      "Epoch 293/500, Loss: 0.03960327431559563, Test Accuracy: 0.7376\n",
      "Epoch 294/500, Loss: 0.05099597945809364, Test Accuracy: 0.7376\n",
      "Epoch 295/500, Loss: 0.037834133952856064, Test Accuracy: 0.7305\n",
      "Epoch 296/500, Loss: 0.0347333587706089, Test Accuracy: 0.7305\n",
      "Epoch 297/500, Loss: 0.03537565842270851, Test Accuracy: 0.7305\n",
      "Epoch 298/500, Loss: 0.0426069051027298, Test Accuracy: 0.7305\n",
      "Epoch 299/500, Loss: 0.04357801005244255, Test Accuracy: 0.7305\n",
      "Epoch 300/500, Loss: 0.038797035813331604, Test Accuracy: 0.7305\n",
      "Epoch 301/500, Loss: 0.048787057399749756, Test Accuracy: 0.7305\n",
      "Epoch 302/500, Loss: 0.04973780736327171, Test Accuracy: 0.7305\n",
      "Epoch 303/500, Loss: 0.036109451204538345, Test Accuracy: 0.7305\n",
      "Epoch 304/500, Loss: 0.03705214709043503, Test Accuracy: 0.7234\n",
      "Epoch 305/500, Loss: 0.03416607156395912, Test Accuracy: 0.7234\n",
      "Epoch 306/500, Loss: 0.03496090695261955, Test Accuracy: 0.7305\n",
      "Epoch 307/500, Loss: 0.04048069566488266, Test Accuracy: 0.7305\n",
      "Epoch 308/500, Loss: 0.037030767649412155, Test Accuracy: 0.7234\n",
      "Epoch 309/500, Loss: 0.036384377628564835, Test Accuracy: 0.7234\n",
      "Epoch 310/500, Loss: 0.03209272027015686, Test Accuracy: 0.7234\n",
      "Epoch 311/500, Loss: 0.03855279088020325, Test Accuracy: 0.7234\n",
      "Epoch 312/500, Loss: 0.035426441580057144, Test Accuracy: 0.7305\n",
      "Epoch 313/500, Loss: 0.056510601192712784, Test Accuracy: 0.7305\n",
      "Epoch 314/500, Loss: 0.031878069043159485, Test Accuracy: 0.7305\n",
      "Epoch 315/500, Loss: 0.04201585426926613, Test Accuracy: 0.7305\n",
      "Early stopping triggered\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch: 215\n",
      "Confusion Matrix:\n",
      " [[54 21]\n",
      " [14 52]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.79      0.72      0.76        75\n",
      "     Class 1       0.71      0.79      0.75        66\n",
      "\n",
      "    accuracy                           0.75       141\n",
      "   macro avg       0.75      0.75      0.75       141\n",
      "weighted avg       0.76      0.75      0.75       141\n",
      "\n",
      "Best Test Accuracy: 0.7518\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 设置随机种子\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # 为所有GPU设置种子\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载下游任务数据集\n",
    "file_path = 'data2.csv'\n",
    "data2_df = pd.read_csv(file_path)\n",
    "\n",
    "# 将数据集拆分为训练集和测试集\n",
    "train_df, test_df = train_test_split(data2_df, test_size=0.2, random_state=seed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 使用上游模型生成增强表示的函数\n",
    "def generate_enhanced_embeddings(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.hidden_states[-1][:, 0, :]\n",
    "    predictions = torch.sigmoid(outputs.logits)\n",
    "    enhanced_embeddings = torch.cat((embeddings, predictions), dim=1)\n",
    "    return enhanced_embeddings\n",
    "\n",
    "# 使用上游模型生成训练集和测试集的增强表示\n",
    "model_path = \"saved_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "config.num_labels = 38  # 手动设置标签数量为 21\n",
    "config.output_hidden_states = True\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, config=config).to(device)\n",
    "\n",
    "# 定义简单的二分类器\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# 创建分类器实例\n",
    "input_dim = config.hidden_size + config.num_labels\n",
    "classifier = SimpleClassifier(input_dim).to(device)\n",
    "\n",
    "# 将模型和分类器的参数放在同一个优化器中，并添加L2正则化\n",
    "optimizer = optim.Adam(list(model.parameters()) + list(classifier.parameters()), lr=5e-5, weight_decay=1e-4)\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 记录损失值和测试集准确度\n",
    "train_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "# 训练模型和分类器\n",
    "num_epochs = 500\n",
    "patience = 100  # 早停法的耐心值\n",
    "best_accuracy = 0.0\n",
    "best_epoch = 0\n",
    "best_model_state = None\n",
    "best_classifier_state = None\n",
    "early_stop_counter = 0\n",
    "\n",
    "model.train()\n",
    "classifier.train()\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    train_embeddings = generate_enhanced_embeddings(train_df['compound'].tolist(), tokenizer, model)\n",
    "    outputs = classifier(train_embeddings).squeeze()\n",
    "    train_labels = torch.tensor(train_df['label'].values, dtype=torch.float).to(device)\n",
    "    loss = criterion(outputs, train_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    # 计算测试集上的准确度\n",
    "    model.eval()\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        test_embeddings = generate_enhanced_embeddings(test_df['compound'].tolist(), tokenizer, model)\n",
    "        test_outputs = classifier(test_embeddings).squeeze()\n",
    "        test_labels = torch.tensor(test_df['label'].values, dtype=torch.float).to(device)\n",
    "        test_predictions = torch.sigmoid(test_outputs) > 0.5\n",
    "        accuracy = accuracy_score(test_labels.cpu(), test_predictions.cpu())\n",
    "        test_accuracies.append(accuracy)\n",
    "        \n",
    "        # 记录最好的模型\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_epoch = epoch + 1\n",
    "            torch.save(model.state_dict(),\"best_model_state.pth\")\n",
    "            torch.save(classifier.state_dict(),\"best_classifier_state.pth\")\n",
    "            early_stop_counter = 0  # 重置早停计数器\n",
    "        else:\n",
    "            early_stop_counter += 1  # 增加早停计数器\n",
    "    \n",
    "    model.train()\n",
    "    classifier.train()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}, Test Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # 早停条件满足时停止训练\n",
    "    if early_stop_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "# 绘制训练损失减少过程\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 加载测试集上效果最好的模型参数\n",
    "model.load_state_dict(torch.load(\"best_model_state.pth\", weights_only=True))\n",
    "classifier.load_state_dict(torch.load(\"best_classifier_state.pth\", weights_only=True))\n",
    "\n",
    "# 在测试集上评估最好的模型\n",
    "model.eval()\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    test_embeddings = generate_enhanced_embeddings(test_df['compound'].tolist(), tokenizer, model)\n",
    "    test_outputs = classifier(test_embeddings).squeeze()\n",
    "    test_labels = torch.tensor(test_df['label'].values, dtype=torch.float).to(device)\n",
    "    test_predictions = torch.sigmoid(test_outputs) > 0.5\n",
    "\n",
    "# 计算混淆矩阵和其他评价指标\n",
    "conf_matrix = confusion_matrix(test_labels.cpu(), test_predictions.cpu())\n",
    "class_report = classification_report(test_labels.cpu(), test_predictions.cpu(), target_names=['Class 0', 'Class 1'])\n",
    "accuracy = accuracy_score(test_labels.cpu(), test_predictions.cpu())\n",
    "\n",
    "print(\"Best Epoch:\", best_epoch)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", class_report)\n",
    "print(f\"Best Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec753de-2afa-4e2b-a802-3ff1f3330758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0c28ba60-7121-4255-b08c-0497c94b2c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#十倍交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bf35e378-934c-481a-bb05-ae72334c5930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/500, Loss: 0.7126027941703796, Test Accuracy: 0.4507\n",
      "Epoch 2/500, Loss: 0.7075218558311462, Test Accuracy: 0.4648\n",
      "Epoch 3/500, Loss: 0.7068123817443848, Test Accuracy: 0.4648\n",
      "Epoch 4/500, Loss: 0.7088564038276672, Test Accuracy: 0.4648\n",
      "Epoch 5/500, Loss: 0.7058301568031311, Test Accuracy: 0.4789\n",
      "Epoch 6/500, Loss: 0.7012085318565369, Test Accuracy: 0.4930\n",
      "Epoch 7/500, Loss: 0.6976335048675537, Test Accuracy: 0.4930\n",
      "Epoch 8/500, Loss: 0.6914361715316772, Test Accuracy: 0.4648\n",
      "Epoch 9/500, Loss: 0.6967139840126038, Test Accuracy: 0.4789\n",
      "Epoch 10/500, Loss: 0.6897000670433044, Test Accuracy: 0.4648\n",
      "Epoch 11/500, Loss: 0.6894508004188538, Test Accuracy: 0.4648\n",
      "Epoch 12/500, Loss: 0.6874242424964905, Test Accuracy: 0.4930\n",
      "Epoch 13/500, Loss: 0.6824612617492676, Test Accuracy: 0.4930\n",
      "Epoch 14/500, Loss: 0.6813135743141174, Test Accuracy: 0.4930\n",
      "Epoch 15/500, Loss: 0.6760850548744202, Test Accuracy: 0.5211\n",
      "Epoch 16/500, Loss: 0.6756823062896729, Test Accuracy: 0.5211\n",
      "Epoch 17/500, Loss: 0.6698163151741028, Test Accuracy: 0.5211\n",
      "Epoch 18/500, Loss: 0.667187511920929, Test Accuracy: 0.5352\n",
      "Epoch 19/500, Loss: 0.6730313897132874, Test Accuracy: 0.5493\n",
      "Epoch 20/500, Loss: 0.6580749154090881, Test Accuracy: 0.5493\n",
      "Epoch 21/500, Loss: 0.6669975519180298, Test Accuracy: 0.5775\n",
      "Epoch 22/500, Loss: 0.6663844585418701, Test Accuracy: 0.5915\n",
      "Epoch 23/500, Loss: 0.660283088684082, Test Accuracy: 0.5915\n",
      "Epoch 24/500, Loss: 0.6561428308486938, Test Accuracy: 0.5915\n",
      "Epoch 25/500, Loss: 0.6511416435241699, Test Accuracy: 0.6197\n",
      "Epoch 26/500, Loss: 0.6583478450775146, Test Accuracy: 0.6338\n",
      "Epoch 27/500, Loss: 0.6553058624267578, Test Accuracy: 0.6338\n",
      "Epoch 28/500, Loss: 0.6471720337867737, Test Accuracy: 0.6338\n",
      "Epoch 29/500, Loss: 0.6451637148857117, Test Accuracy: 0.6338\n",
      "Epoch 30/500, Loss: 0.6494373083114624, Test Accuracy: 0.6338\n",
      "Epoch 31/500, Loss: 0.6435286998748779, Test Accuracy: 0.6338\n",
      "Epoch 32/500, Loss: 0.6390079855918884, Test Accuracy: 0.6197\n",
      "Epoch 33/500, Loss: 0.6379466652870178, Test Accuracy: 0.6197\n",
      "Epoch 34/500, Loss: 0.6375837922096252, Test Accuracy: 0.6056\n",
      "Epoch 35/500, Loss: 0.6340683102607727, Test Accuracy: 0.6056\n",
      "Epoch 36/500, Loss: 0.6334123611450195, Test Accuracy: 0.6056\n",
      "Epoch 37/500, Loss: 0.6321490406990051, Test Accuracy: 0.6197\n",
      "Epoch 38/500, Loss: 0.6238609552383423, Test Accuracy: 0.6197\n",
      "Epoch 39/500, Loss: 0.6302588582038879, Test Accuracy: 0.6338\n",
      "Epoch 40/500, Loss: 0.6234397888183594, Test Accuracy: 0.6338\n",
      "Epoch 41/500, Loss: 0.624638557434082, Test Accuracy: 0.6197\n",
      "Epoch 42/500, Loss: 0.6162288784980774, Test Accuracy: 0.6197\n",
      "Epoch 43/500, Loss: 0.6159353852272034, Test Accuracy: 0.6197\n",
      "Epoch 44/500, Loss: 0.6106510758399963, Test Accuracy: 0.6056\n",
      "Epoch 45/500, Loss: 0.6092538237571716, Test Accuracy: 0.6056\n",
      "Epoch 46/500, Loss: 0.6092888116836548, Test Accuracy: 0.6056\n",
      "Epoch 47/500, Loss: 0.6111444234848022, Test Accuracy: 0.6056\n",
      "Epoch 48/500, Loss: 0.608014702796936, Test Accuracy: 0.6056\n",
      "Epoch 49/500, Loss: 0.5999434590339661, Test Accuracy: 0.6056\n",
      "Epoch 50/500, Loss: 0.6031197905540466, Test Accuracy: 0.6056\n",
      "Epoch 51/500, Loss: 0.5966150760650635, Test Accuracy: 0.6056\n",
      "Epoch 52/500, Loss: 0.5973870158195496, Test Accuracy: 0.5915\n",
      "Epoch 53/500, Loss: 0.5927218794822693, Test Accuracy: 0.5915\n",
      "Epoch 54/500, Loss: 0.5962561368942261, Test Accuracy: 0.6056\n",
      "Epoch 55/500, Loss: 0.5852495431900024, Test Accuracy: 0.6056\n",
      "Epoch 56/500, Loss: 0.5880393981933594, Test Accuracy: 0.6056\n",
      "Epoch 57/500, Loss: 0.5817808508872986, Test Accuracy: 0.6056\n",
      "Epoch 58/500, Loss: 0.5907966494560242, Test Accuracy: 0.5915\n",
      "Epoch 59/500, Loss: 0.5762924551963806, Test Accuracy: 0.6056\n",
      "Epoch 60/500, Loss: 0.5703249573707581, Test Accuracy: 0.6197\n",
      "Epoch 61/500, Loss: 0.5684177875518799, Test Accuracy: 0.6197\n",
      "Epoch 62/500, Loss: 0.5686459541320801, Test Accuracy: 0.6197\n",
      "Epoch 63/500, Loss: 0.5705265402793884, Test Accuracy: 0.6197\n",
      "Epoch 64/500, Loss: 0.565345287322998, Test Accuracy: 0.6197\n",
      "Epoch 65/500, Loss: 0.5550533533096313, Test Accuracy: 0.6197\n",
      "Epoch 66/500, Loss: 0.5523136258125305, Test Accuracy: 0.6197\n",
      "Epoch 67/500, Loss: 0.557045578956604, Test Accuracy: 0.6197\n",
      "Epoch 68/500, Loss: 0.5485966205596924, Test Accuracy: 0.6197\n",
      "Epoch 69/500, Loss: 0.5514518022537231, Test Accuracy: 0.6197\n",
      "Epoch 70/500, Loss: 0.547261118888855, Test Accuracy: 0.6197\n",
      "Epoch 71/500, Loss: 0.5431962013244629, Test Accuracy: 0.6197\n",
      "Epoch 72/500, Loss: 0.5427314639091492, Test Accuracy: 0.6338\n",
      "Epoch 73/500, Loss: 0.5355791449546814, Test Accuracy: 0.6479\n",
      "Epoch 74/500, Loss: 0.5393935441970825, Test Accuracy: 0.6479\n",
      "Epoch 75/500, Loss: 0.526949405670166, Test Accuracy: 0.6620\n",
      "Epoch 76/500, Loss: 0.5244016051292419, Test Accuracy: 0.6620\n",
      "Epoch 77/500, Loss: 0.5389603972434998, Test Accuracy: 0.6761\n",
      "Epoch 78/500, Loss: 0.5166929960250854, Test Accuracy: 0.6761\n",
      "Epoch 79/500, Loss: 0.5232612490653992, Test Accuracy: 0.6761\n",
      "Epoch 80/500, Loss: 0.5206447243690491, Test Accuracy: 0.6761\n",
      "Epoch 81/500, Loss: 0.5217832326889038, Test Accuracy: 0.6901\n",
      "Epoch 82/500, Loss: 0.5119624137878418, Test Accuracy: 0.6901\n",
      "Epoch 83/500, Loss: 0.5064847469329834, Test Accuracy: 0.6901\n",
      "Epoch 84/500, Loss: 0.5107471346855164, Test Accuracy: 0.6901\n",
      "Epoch 85/500, Loss: 0.5046408772468567, Test Accuracy: 0.6901\n",
      "Epoch 86/500, Loss: 0.5051385164260864, Test Accuracy: 0.6901\n",
      "Epoch 87/500, Loss: 0.49533596634864807, Test Accuracy: 0.6901\n",
      "Epoch 88/500, Loss: 0.495819628238678, Test Accuracy: 0.6901\n",
      "Epoch 89/500, Loss: 0.4959854781627655, Test Accuracy: 0.6901\n",
      "Epoch 90/500, Loss: 0.4822845757007599, Test Accuracy: 0.6901\n",
      "Epoch 91/500, Loss: 0.482229620218277, Test Accuracy: 0.6901\n",
      "Epoch 92/500, Loss: 0.4816170036792755, Test Accuracy: 0.6901\n",
      "Epoch 93/500, Loss: 0.4798324704170227, Test Accuracy: 0.6761\n",
      "Epoch 94/500, Loss: 0.47184351086616516, Test Accuracy: 0.7042\n",
      "Epoch 95/500, Loss: 0.4623696506023407, Test Accuracy: 0.7042\n",
      "Epoch 96/500, Loss: 0.471782386302948, Test Accuracy: 0.7042\n",
      "Epoch 97/500, Loss: 0.463846892118454, Test Accuracy: 0.7042\n",
      "Epoch 98/500, Loss: 0.45888960361480713, Test Accuracy: 0.7042\n",
      "Epoch 99/500, Loss: 0.4700982868671417, Test Accuracy: 0.7042\n",
      "Epoch 100/500, Loss: 0.45633652806282043, Test Accuracy: 0.7042\n",
      "Epoch 101/500, Loss: 0.4540676176548004, Test Accuracy: 0.7183\n",
      "Epoch 102/500, Loss: 0.44734084606170654, Test Accuracy: 0.7183\n",
      "Epoch 103/500, Loss: 0.4433370530605316, Test Accuracy: 0.7465\n",
      "Epoch 104/500, Loss: 0.4303535521030426, Test Accuracy: 0.7465\n",
      "Epoch 105/500, Loss: 0.4551272690296173, Test Accuracy: 0.7324\n",
      "Epoch 106/500, Loss: 0.4263197183609009, Test Accuracy: 0.7183\n",
      "Epoch 107/500, Loss: 0.43374741077423096, Test Accuracy: 0.7183\n",
      "Epoch 108/500, Loss: 0.4195231795310974, Test Accuracy: 0.7183\n",
      "Epoch 109/500, Loss: 0.41822442412376404, Test Accuracy: 0.7183\n",
      "Epoch 110/500, Loss: 0.4188150465488434, Test Accuracy: 0.7183\n",
      "Epoch 111/500, Loss: 0.41650325059890747, Test Accuracy: 0.7183\n",
      "Epoch 112/500, Loss: 0.4264184236526489, Test Accuracy: 0.7183\n",
      "Epoch 113/500, Loss: 0.40729212760925293, Test Accuracy: 0.7324\n",
      "Epoch 114/500, Loss: 0.4080994427204132, Test Accuracy: 0.7324\n",
      "Epoch 115/500, Loss: 0.41570013761520386, Test Accuracy: 0.7324\n",
      "Epoch 116/500, Loss: 0.39475297927856445, Test Accuracy: 0.7183\n",
      "Epoch 117/500, Loss: 0.39087238907814026, Test Accuracy: 0.7183\n",
      "Epoch 118/500, Loss: 0.3932855427265167, Test Accuracy: 0.7183\n",
      "Epoch 119/500, Loss: 0.3856239914894104, Test Accuracy: 0.7183\n",
      "Epoch 120/500, Loss: 0.3807365298271179, Test Accuracy: 0.7183\n",
      "Epoch 121/500, Loss: 0.37549692392349243, Test Accuracy: 0.7183\n",
      "Epoch 122/500, Loss: 0.37922173738479614, Test Accuracy: 0.7324\n",
      "Epoch 123/500, Loss: 0.3695732653141022, Test Accuracy: 0.7324\n",
      "Epoch 124/500, Loss: 0.37293171882629395, Test Accuracy: 0.7183\n",
      "Epoch 125/500, Loss: 0.35371747612953186, Test Accuracy: 0.7183\n",
      "Epoch 126/500, Loss: 0.3418022692203522, Test Accuracy: 0.7183\n",
      "Epoch 127/500, Loss: 0.36709651350975037, Test Accuracy: 0.7183\n",
      "Epoch 128/500, Loss: 0.3649982511997223, Test Accuracy: 0.7183\n",
      "Epoch 129/500, Loss: 0.34663280844688416, Test Accuracy: 0.7183\n",
      "Epoch 130/500, Loss: 0.34306371212005615, Test Accuracy: 0.7183\n",
      "Epoch 131/500, Loss: 0.3390389680862427, Test Accuracy: 0.7183\n",
      "Epoch 132/500, Loss: 0.343988299369812, Test Accuracy: 0.7183\n",
      "Epoch 133/500, Loss: 0.32530245184898376, Test Accuracy: 0.7183\n",
      "Epoch 134/500, Loss: 0.33238083124160767, Test Accuracy: 0.7183\n",
      "Epoch 135/500, Loss: 0.31657883524894714, Test Accuracy: 0.7183\n",
      "Epoch 136/500, Loss: 0.3093241751194, Test Accuracy: 0.7183\n",
      "Epoch 137/500, Loss: 0.31711453199386597, Test Accuracy: 0.7183\n",
      "Epoch 138/500, Loss: 0.3237883448600769, Test Accuracy: 0.7183\n",
      "Epoch 139/500, Loss: 0.30300605297088623, Test Accuracy: 0.7183\n",
      "Epoch 140/500, Loss: 0.3023759424686432, Test Accuracy: 0.7183\n",
      "Epoch 141/500, Loss: 0.3068268299102783, Test Accuracy: 0.7183\n",
      "Epoch 142/500, Loss: 0.29751771688461304, Test Accuracy: 0.7183\n",
      "Epoch 143/500, Loss: 0.3024914562702179, Test Accuracy: 0.7324\n",
      "Epoch 144/500, Loss: 0.308172345161438, Test Accuracy: 0.7183\n",
      "Epoch 145/500, Loss: 0.30334797501564026, Test Accuracy: 0.7183\n",
      "Epoch 146/500, Loss: 0.2947419285774231, Test Accuracy: 0.7183\n",
      "Epoch 147/500, Loss: 0.28948384523391724, Test Accuracy: 0.7324\n",
      "Epoch 148/500, Loss: 0.25900301337242126, Test Accuracy: 0.7324\n",
      "Epoch 149/500, Loss: 0.2854965627193451, Test Accuracy: 0.7324\n",
      "Epoch 150/500, Loss: 0.2636955976486206, Test Accuracy: 0.7324\n",
      "Epoch 151/500, Loss: 0.2660391330718994, Test Accuracy: 0.7183\n",
      "Epoch 152/500, Loss: 0.2702161967754364, Test Accuracy: 0.7183\n",
      "Epoch 153/500, Loss: 0.26282817125320435, Test Accuracy: 0.7042\n",
      "Epoch 154/500, Loss: 0.27293968200683594, Test Accuracy: 0.7042\n",
      "Epoch 155/500, Loss: 0.2717570662498474, Test Accuracy: 0.7183\n",
      "Epoch 156/500, Loss: 0.2590615153312683, Test Accuracy: 0.7183\n",
      "Epoch 157/500, Loss: 0.25039246678352356, Test Accuracy: 0.7183\n",
      "Epoch 158/500, Loss: 0.25502222776412964, Test Accuracy: 0.7183\n",
      "Epoch 159/500, Loss: 0.24123463034629822, Test Accuracy: 0.7183\n",
      "Epoch 160/500, Loss: 0.2258281111717224, Test Accuracy: 0.7183\n",
      "Epoch 161/500, Loss: 0.23797674477100372, Test Accuracy: 0.7183\n",
      "Epoch 162/500, Loss: 0.2391238808631897, Test Accuracy: 0.7183\n",
      "Epoch 163/500, Loss: 0.2286032736301422, Test Accuracy: 0.7324\n",
      "Epoch 164/500, Loss: 0.216276615858078, Test Accuracy: 0.7324\n",
      "Epoch 165/500, Loss: 0.22560900449752808, Test Accuracy: 0.7324\n",
      "Epoch 166/500, Loss: 0.22306248545646667, Test Accuracy: 0.7324\n",
      "Epoch 167/500, Loss: 0.2059515118598938, Test Accuracy: 0.7324\n",
      "Epoch 168/500, Loss: 0.21783356368541718, Test Accuracy: 0.7324\n",
      "Epoch 169/500, Loss: 0.2092941850423813, Test Accuracy: 0.7324\n",
      "Epoch 170/500, Loss: 0.20161032676696777, Test Accuracy: 0.7324\n",
      "Epoch 171/500, Loss: 0.22420014441013336, Test Accuracy: 0.7324\n",
      "Epoch 172/500, Loss: 0.20782099664211273, Test Accuracy: 0.7465\n",
      "Epoch 173/500, Loss: 0.21312570571899414, Test Accuracy: 0.7465\n",
      "Epoch 174/500, Loss: 0.19310259819030762, Test Accuracy: 0.7465\n",
      "Epoch 175/500, Loss: 0.19816096127033234, Test Accuracy: 0.7465\n",
      "Epoch 176/500, Loss: 0.19386343657970428, Test Accuracy: 0.7324\n",
      "Epoch 177/500, Loss: 0.1965055614709854, Test Accuracy: 0.7324\n",
      "Epoch 178/500, Loss: 0.19024263322353363, Test Accuracy: 0.7324\n",
      "Epoch 179/500, Loss: 0.18992340564727783, Test Accuracy: 0.7183\n",
      "Epoch 180/500, Loss: 0.1620892733335495, Test Accuracy: 0.7183\n",
      "Epoch 181/500, Loss: 0.18692877888679504, Test Accuracy: 0.7183\n",
      "Epoch 182/500, Loss: 0.17176592350006104, Test Accuracy: 0.7183\n",
      "Epoch 183/500, Loss: 0.18349140882492065, Test Accuracy: 0.7183\n",
      "Epoch 184/500, Loss: 0.16427108645439148, Test Accuracy: 0.7183\n",
      "Epoch 185/500, Loss: 0.18731309473514557, Test Accuracy: 0.7183\n",
      "Epoch 186/500, Loss: 0.1694309562444687, Test Accuracy: 0.7324\n",
      "Epoch 187/500, Loss: 0.18049199879169464, Test Accuracy: 0.7324\n",
      "Epoch 188/500, Loss: 0.18266445398330688, Test Accuracy: 0.7324\n",
      "Epoch 189/500, Loss: 0.15800108015537262, Test Accuracy: 0.7324\n",
      "Epoch 190/500, Loss: 0.15617544949054718, Test Accuracy: 0.7183\n",
      "Epoch 191/500, Loss: 0.1447463482618332, Test Accuracy: 0.7183\n",
      "Epoch 192/500, Loss: 0.14889377355575562, Test Accuracy: 0.7465\n",
      "Epoch 193/500, Loss: 0.15038886666297913, Test Accuracy: 0.7465\n",
      "Epoch 194/500, Loss: 0.15741902589797974, Test Accuracy: 0.7465\n",
      "Epoch 195/500, Loss: 0.15383094549179077, Test Accuracy: 0.7324\n",
      "Epoch 196/500, Loss: 0.15263579785823822, Test Accuracy: 0.7183\n",
      "Epoch 197/500, Loss: 0.15919849276542664, Test Accuracy: 0.7183\n",
      "Epoch 198/500, Loss: 0.1375477910041809, Test Accuracy: 0.7183\n",
      "Epoch 199/500, Loss: 0.15461471676826477, Test Accuracy: 0.7183\n",
      "Epoch 200/500, Loss: 0.13326452672481537, Test Accuracy: 0.7183\n",
      "Epoch 201/500, Loss: 0.13838663697242737, Test Accuracy: 0.7183\n",
      "Epoch 202/500, Loss: 0.13838016986846924, Test Accuracy: 0.7183\n",
      "Epoch 203/500, Loss: 0.138541579246521, Test Accuracy: 0.7183\n",
      "Early stopping triggered\n",
      "Best Epoch: 103\n",
      "Confusion Matrix:\n",
      " [[24 13]\n",
      " [ 7 27]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.77      0.65      0.71        37\n",
      "     Class 1       0.68      0.79      0.73        34\n",
      "\n",
      "    accuracy                           0.72        71\n",
      "   macro avg       0.72      0.72      0.72        71\n",
      "weighted avg       0.73      0.72      0.72        71\n",
      "\n",
      "Best Test Accuracy: 0.7183\n",
      "Fold 2\n",
      "Epoch 1/500, Loss: 0.5669850707054138, Test Accuracy: 0.9014\n",
      "Epoch 2/500, Loss: 0.5561668872833252, Test Accuracy: 0.9155\n",
      "Epoch 3/500, Loss: 0.5433814525604248, Test Accuracy: 0.9155\n",
      "Epoch 4/500, Loss: 0.534198522567749, Test Accuracy: 0.9155\n",
      "Epoch 5/500, Loss: 0.5198304057121277, Test Accuracy: 0.9155\n",
      "Epoch 6/500, Loss: 0.49817079305648804, Test Accuracy: 0.9296\n",
      "Epoch 7/500, Loss: 0.4910389184951782, Test Accuracy: 0.9155\n",
      "Epoch 8/500, Loss: 0.4773465096950531, Test Accuracy: 0.9437\n",
      "Epoch 9/500, Loss: 0.46175912022590637, Test Accuracy: 0.9437\n",
      "Epoch 10/500, Loss: 0.44387099146842957, Test Accuracy: 0.9577\n",
      "Epoch 11/500, Loss: 0.4403526484966278, Test Accuracy: 0.9577\n",
      "Epoch 12/500, Loss: 0.4227796792984009, Test Accuracy: 0.9577\n",
      "Epoch 13/500, Loss: 0.41897153854370117, Test Accuracy: 0.9577\n",
      "Epoch 14/500, Loss: 0.4204367697238922, Test Accuracy: 0.9577\n",
      "Epoch 15/500, Loss: 0.4038729667663574, Test Accuracy: 0.9577\n",
      "Epoch 16/500, Loss: 0.4164489507675171, Test Accuracy: 0.9437\n",
      "Epoch 17/500, Loss: 0.3986436724662781, Test Accuracy: 0.9577\n",
      "Epoch 18/500, Loss: 0.384315550327301, Test Accuracy: 0.9577\n",
      "Epoch 19/500, Loss: 0.3793486952781677, Test Accuracy: 0.9577\n",
      "Epoch 20/500, Loss: 0.3814697861671448, Test Accuracy: 0.9577\n",
      "Epoch 21/500, Loss: 0.35676053166389465, Test Accuracy: 0.9577\n",
      "Epoch 22/500, Loss: 0.3509165346622467, Test Accuracy: 0.9577\n",
      "Epoch 23/500, Loss: 0.3602381646633148, Test Accuracy: 0.9577\n",
      "Epoch 24/500, Loss: 0.3387065529823303, Test Accuracy: 0.9577\n",
      "Epoch 25/500, Loss: 0.34273087978363037, Test Accuracy: 0.9577\n",
      "Epoch 26/500, Loss: 0.33177945017814636, Test Accuracy: 0.9577\n",
      "Epoch 27/500, Loss: 0.32382121682167053, Test Accuracy: 0.9577\n",
      "Epoch 28/500, Loss: 0.32273977994918823, Test Accuracy: 0.9577\n",
      "Epoch 29/500, Loss: 0.32009512186050415, Test Accuracy: 0.9437\n",
      "Epoch 30/500, Loss: 0.3077526092529297, Test Accuracy: 0.9437\n",
      "Epoch 31/500, Loss: 0.31879693269729614, Test Accuracy: 0.9437\n",
      "Epoch 32/500, Loss: 0.29230889678001404, Test Accuracy: 0.9296\n",
      "Epoch 33/500, Loss: 0.3032042384147644, Test Accuracy: 0.9296\n",
      "Epoch 34/500, Loss: 0.28481870889663696, Test Accuracy: 0.9296\n",
      "Epoch 35/500, Loss: 0.2887765169143677, Test Accuracy: 0.9437\n",
      "Epoch 36/500, Loss: 0.2733394503593445, Test Accuracy: 0.9437\n",
      "Epoch 37/500, Loss: 0.2882235646247864, Test Accuracy: 0.9437\n",
      "Epoch 38/500, Loss: 0.2834283113479614, Test Accuracy: 0.9437\n",
      "Epoch 39/500, Loss: 0.2857515215873718, Test Accuracy: 0.9437\n",
      "Epoch 40/500, Loss: 0.26491832733154297, Test Accuracy: 0.9437\n",
      "Epoch 41/500, Loss: 0.2736978828907013, Test Accuracy: 0.9437\n",
      "Epoch 42/500, Loss: 0.25326263904571533, Test Accuracy: 0.9437\n",
      "Epoch 43/500, Loss: 0.25521162152290344, Test Accuracy: 0.9437\n",
      "Epoch 44/500, Loss: 0.2511391341686249, Test Accuracy: 0.9437\n",
      "Epoch 45/500, Loss: 0.24853231012821198, Test Accuracy: 0.9437\n",
      "Epoch 46/500, Loss: 0.24268151819705963, Test Accuracy: 0.9437\n",
      "Epoch 47/500, Loss: 0.2521889805793762, Test Accuracy: 0.9437\n",
      "Epoch 48/500, Loss: 0.2592966854572296, Test Accuracy: 0.9437\n",
      "Epoch 49/500, Loss: 0.23551635444164276, Test Accuracy: 0.9437\n",
      "Epoch 50/500, Loss: 0.2344205677509308, Test Accuracy: 0.9437\n",
      "Epoch 51/500, Loss: 0.23161564767360687, Test Accuracy: 0.9437\n",
      "Epoch 52/500, Loss: 0.22322186827659607, Test Accuracy: 0.9437\n",
      "Epoch 53/500, Loss: 0.23434241116046906, Test Accuracy: 0.9437\n",
      "Epoch 54/500, Loss: 0.2372567504644394, Test Accuracy: 0.9437\n",
      "Epoch 55/500, Loss: 0.20701679587364197, Test Accuracy: 0.9437\n",
      "Epoch 56/500, Loss: 0.21582971513271332, Test Accuracy: 0.9437\n",
      "Epoch 57/500, Loss: 0.2145816534757614, Test Accuracy: 0.9437\n",
      "Epoch 58/500, Loss: 0.21393048763275146, Test Accuracy: 0.9437\n",
      "Epoch 59/500, Loss: 0.2089957743883133, Test Accuracy: 0.9437\n",
      "Epoch 60/500, Loss: 0.2146247774362564, Test Accuracy: 0.9437\n",
      "Epoch 61/500, Loss: 0.20350565016269684, Test Accuracy: 0.9437\n",
      "Epoch 62/500, Loss: 0.1893380880355835, Test Accuracy: 0.9437\n",
      "Epoch 63/500, Loss: 0.1998278945684433, Test Accuracy: 0.9437\n",
      "Epoch 64/500, Loss: 0.18421101570129395, Test Accuracy: 0.9437\n",
      "Epoch 65/500, Loss: 0.1806236207485199, Test Accuracy: 0.9437\n",
      "Epoch 66/500, Loss: 0.2003595232963562, Test Accuracy: 0.9437\n",
      "Epoch 67/500, Loss: 0.2013622224330902, Test Accuracy: 0.9577\n",
      "Epoch 68/500, Loss: 0.1893371343612671, Test Accuracy: 0.9577\n",
      "Epoch 69/500, Loss: 0.19427412748336792, Test Accuracy: 0.9577\n",
      "Epoch 70/500, Loss: 0.1887269914150238, Test Accuracy: 0.9577\n",
      "Epoch 71/500, Loss: 0.17966146767139435, Test Accuracy: 0.9437\n",
      "Epoch 72/500, Loss: 0.1715651899576187, Test Accuracy: 0.9437\n",
      "Epoch 73/500, Loss: 0.18603910505771637, Test Accuracy: 0.9437\n",
      "Epoch 74/500, Loss: 0.1741102635860443, Test Accuracy: 0.9437\n",
      "Epoch 75/500, Loss: 0.1733490228652954, Test Accuracy: 0.9437\n",
      "Epoch 76/500, Loss: 0.158391073346138, Test Accuracy: 0.9437\n",
      "Epoch 77/500, Loss: 0.1687486320734024, Test Accuracy: 0.9437\n",
      "Epoch 78/500, Loss: 0.1578323096036911, Test Accuracy: 0.9437\n",
      "Epoch 79/500, Loss: 0.16071367263793945, Test Accuracy: 0.9437\n",
      "Epoch 80/500, Loss: 0.16209189593791962, Test Accuracy: 0.9577\n",
      "Epoch 81/500, Loss: 0.14931370317935944, Test Accuracy: 0.9577\n",
      "Epoch 82/500, Loss: 0.17232561111450195, Test Accuracy: 0.9577\n",
      "Epoch 83/500, Loss: 0.15171879529953003, Test Accuracy: 0.9577\n",
      "Epoch 84/500, Loss: 0.15378578007221222, Test Accuracy: 0.9577\n",
      "Epoch 85/500, Loss: 0.15252333879470825, Test Accuracy: 0.9437\n",
      "Epoch 86/500, Loss: 0.1546587347984314, Test Accuracy: 0.9437\n",
      "Epoch 87/500, Loss: 0.1521410197019577, Test Accuracy: 0.9437\n",
      "Epoch 88/500, Loss: 0.1365353763103485, Test Accuracy: 0.9437\n",
      "Epoch 89/500, Loss: 0.13316692411899567, Test Accuracy: 0.9437\n",
      "Epoch 90/500, Loss: 0.14002354443073273, Test Accuracy: 0.9437\n",
      "Epoch 91/500, Loss: 0.13735181093215942, Test Accuracy: 0.9577\n",
      "Epoch 92/500, Loss: 0.13533024489879608, Test Accuracy: 0.9577\n",
      "Epoch 93/500, Loss: 0.1298929899930954, Test Accuracy: 0.9437\n",
      "Epoch 94/500, Loss: 0.15472790598869324, Test Accuracy: 0.9437\n",
      "Epoch 95/500, Loss: 0.14543654024600983, Test Accuracy: 0.9577\n",
      "Epoch 96/500, Loss: 0.11977749317884445, Test Accuracy: 0.9577\n",
      "Epoch 97/500, Loss: 0.1277141124010086, Test Accuracy: 0.9577\n",
      "Epoch 98/500, Loss: 0.12811946868896484, Test Accuracy: 0.9577\n",
      "Epoch 99/500, Loss: 0.12025254219770432, Test Accuracy: 0.9577\n",
      "Epoch 100/500, Loss: 0.11002187430858612, Test Accuracy: 0.9577\n",
      "Epoch 101/500, Loss: 0.1256658434867859, Test Accuracy: 0.9577\n",
      "Epoch 102/500, Loss: 0.11399925500154495, Test Accuracy: 0.9437\n",
      "Epoch 103/500, Loss: 0.1387351006269455, Test Accuracy: 0.9437\n",
      "Epoch 104/500, Loss: 0.13505616784095764, Test Accuracy: 0.9577\n",
      "Epoch 105/500, Loss: 0.12834647297859192, Test Accuracy: 0.9577\n",
      "Epoch 106/500, Loss: 0.12232684344053268, Test Accuracy: 0.9577\n",
      "Epoch 107/500, Loss: 0.11589623987674713, Test Accuracy: 0.9577\n",
      "Epoch 108/500, Loss: 0.11037075519561768, Test Accuracy: 0.9577\n",
      "Epoch 109/500, Loss: 0.11504215747117996, Test Accuracy: 0.9577\n",
      "Epoch 110/500, Loss: 0.09786015003919601, Test Accuracy: 0.9577\n",
      "Early stopping triggered\n",
      "Best Epoch: 10\n",
      "Confusion Matrix:\n",
      " [[36  2]\n",
      " [ 1 32]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.97      0.95      0.96        38\n",
      "     Class 1       0.94      0.97      0.96        33\n",
      "\n",
      "    accuracy                           0.96        71\n",
      "   macro avg       0.96      0.96      0.96        71\n",
      "weighted avg       0.96      0.96      0.96        71\n",
      "\n",
      "Best Test Accuracy: 0.9577\n",
      "Fold 3\n",
      "Epoch 1/500, Loss: 0.6016074419021606, Test Accuracy: 0.7887\n",
      "Epoch 2/500, Loss: 0.5645936727523804, Test Accuracy: 0.8451\n",
      "Epoch 3/500, Loss: 0.5477014183998108, Test Accuracy: 0.8451\n",
      "Epoch 4/500, Loss: 0.5054670572280884, Test Accuracy: 0.8592\n",
      "Epoch 5/500, Loss: 0.49869999289512634, Test Accuracy: 0.8732\n",
      "Epoch 6/500, Loss: 0.47615084052085876, Test Accuracy: 0.8732\n",
      "Epoch 7/500, Loss: 0.44554829597473145, Test Accuracy: 0.8873\n",
      "Epoch 8/500, Loss: 0.4435010254383087, Test Accuracy: 0.9014\n",
      "Epoch 9/500, Loss: 0.41527876257896423, Test Accuracy: 0.8732\n",
      "Epoch 10/500, Loss: 0.4002531170845032, Test Accuracy: 0.8732\n",
      "Epoch 11/500, Loss: 0.3788735866546631, Test Accuracy: 0.8732\n",
      "Epoch 12/500, Loss: 0.37107187509536743, Test Accuracy: 0.8732\n",
      "Epoch 13/500, Loss: 0.3599826991558075, Test Accuracy: 0.8732\n",
      "Epoch 14/500, Loss: 0.34562772512435913, Test Accuracy: 0.8732\n",
      "Epoch 15/500, Loss: 0.3266342282295227, Test Accuracy: 0.8873\n",
      "Epoch 16/500, Loss: 0.33038339018821716, Test Accuracy: 0.9014\n",
      "Epoch 17/500, Loss: 0.2954694926738739, Test Accuracy: 0.9014\n",
      "Epoch 18/500, Loss: 0.3006192445755005, Test Accuracy: 0.9014\n",
      "Epoch 19/500, Loss: 0.28174325823783875, Test Accuracy: 0.9014\n",
      "Epoch 20/500, Loss: 0.2609882056713104, Test Accuracy: 0.9014\n",
      "Epoch 21/500, Loss: 0.26790526509284973, Test Accuracy: 0.9014\n",
      "Epoch 22/500, Loss: 0.2733761668205261, Test Accuracy: 0.9014\n",
      "Epoch 23/500, Loss: 0.24122555553913116, Test Accuracy: 0.9155\n",
      "Epoch 24/500, Loss: 0.24476823210716248, Test Accuracy: 0.9155\n",
      "Epoch 25/500, Loss: 0.22833351790905, Test Accuracy: 0.9155\n",
      "Epoch 26/500, Loss: 0.24366112053394318, Test Accuracy: 0.9155\n",
      "Epoch 27/500, Loss: 0.21949662268161774, Test Accuracy: 0.9155\n",
      "Epoch 28/500, Loss: 0.22302667796611786, Test Accuracy: 0.9155\n",
      "Epoch 29/500, Loss: 0.2153308093547821, Test Accuracy: 0.9296\n",
      "Epoch 30/500, Loss: 0.20685504376888275, Test Accuracy: 0.9437\n",
      "Epoch 31/500, Loss: 0.2005392163991928, Test Accuracy: 0.9437\n",
      "Epoch 32/500, Loss: 0.18883979320526123, Test Accuracy: 0.9437\n",
      "Epoch 33/500, Loss: 0.18654005229473114, Test Accuracy: 0.9437\n",
      "Epoch 34/500, Loss: 0.18486982583999634, Test Accuracy: 0.9437\n",
      "Epoch 35/500, Loss: 0.1892395317554474, Test Accuracy: 0.9437\n",
      "Epoch 36/500, Loss: 0.18375855684280396, Test Accuracy: 0.9296\n",
      "Epoch 37/500, Loss: 0.18857358396053314, Test Accuracy: 0.9296\n",
      "Epoch 38/500, Loss: 0.16744691133499146, Test Accuracy: 0.9437\n",
      "Epoch 39/500, Loss: 0.16428443789482117, Test Accuracy: 0.9437\n",
      "Epoch 40/500, Loss: 0.16449984908103943, Test Accuracy: 0.9437\n",
      "Epoch 41/500, Loss: 0.15809528529644012, Test Accuracy: 0.9437\n",
      "Epoch 42/500, Loss: 0.17072266340255737, Test Accuracy: 0.9437\n",
      "Epoch 43/500, Loss: 0.15811683237552643, Test Accuracy: 0.9437\n",
      "Epoch 44/500, Loss: 0.1544778198003769, Test Accuracy: 0.9437\n",
      "Epoch 45/500, Loss: 0.15272323787212372, Test Accuracy: 0.9437\n",
      "Epoch 46/500, Loss: 0.14382699131965637, Test Accuracy: 0.9437\n",
      "Epoch 47/500, Loss: 0.158996120095253, Test Accuracy: 0.9437\n",
      "Epoch 48/500, Loss: 0.13452719151973724, Test Accuracy: 0.9437\n",
      "Epoch 49/500, Loss: 0.13953110575675964, Test Accuracy: 0.9437\n",
      "Epoch 50/500, Loss: 0.13471932709217072, Test Accuracy: 0.9437\n",
      "Epoch 51/500, Loss: 0.14503268897533417, Test Accuracy: 0.9437\n",
      "Epoch 52/500, Loss: 0.12141283601522446, Test Accuracy: 0.9437\n",
      "Epoch 53/500, Loss: 0.12536491453647614, Test Accuracy: 0.9437\n",
      "Epoch 54/500, Loss: 0.12432056665420532, Test Accuracy: 0.9437\n",
      "Epoch 55/500, Loss: 0.1458924561738968, Test Accuracy: 0.9437\n",
      "Epoch 56/500, Loss: 0.1256544440984726, Test Accuracy: 0.9437\n",
      "Epoch 57/500, Loss: 0.12076365202665329, Test Accuracy: 0.9437\n",
      "Epoch 58/500, Loss: 0.11902442574501038, Test Accuracy: 0.9296\n",
      "Epoch 59/500, Loss: 0.1263059377670288, Test Accuracy: 0.9296\n",
      "Epoch 60/500, Loss: 0.12406715750694275, Test Accuracy: 0.9296\n",
      "Epoch 61/500, Loss: 0.12114108353853226, Test Accuracy: 0.9296\n",
      "Epoch 62/500, Loss: 0.12449441105127335, Test Accuracy: 0.9296\n",
      "Epoch 63/500, Loss: 0.10213891416788101, Test Accuracy: 0.9296\n",
      "Epoch 64/500, Loss: 0.12068706750869751, Test Accuracy: 0.9296\n",
      "Epoch 65/500, Loss: 0.10896188020706177, Test Accuracy: 0.9296\n",
      "Epoch 66/500, Loss: 0.10570681840181351, Test Accuracy: 0.9296\n",
      "Epoch 67/500, Loss: 0.10681405663490295, Test Accuracy: 0.9296\n",
      "Epoch 68/500, Loss: 0.10445006191730499, Test Accuracy: 0.9296\n",
      "Epoch 69/500, Loss: 0.10302269458770752, Test Accuracy: 0.9296\n",
      "Epoch 70/500, Loss: 0.09028315544128418, Test Accuracy: 0.9296\n",
      "Epoch 71/500, Loss: 0.09858096390962601, Test Accuracy: 0.9296\n",
      "Epoch 72/500, Loss: 0.10245740413665771, Test Accuracy: 0.9296\n",
      "Epoch 73/500, Loss: 0.10660513490438461, Test Accuracy: 0.9296\n",
      "Epoch 74/500, Loss: 0.09114684909582138, Test Accuracy: 0.9296\n",
      "Epoch 75/500, Loss: 0.09725741297006607, Test Accuracy: 0.9296\n",
      "Epoch 76/500, Loss: 0.09058946371078491, Test Accuracy: 0.9296\n",
      "Epoch 77/500, Loss: 0.07923387736082077, Test Accuracy: 0.9296\n",
      "Epoch 78/500, Loss: 0.09394340962171555, Test Accuracy: 0.9296\n",
      "Epoch 79/500, Loss: 0.09946895390748978, Test Accuracy: 0.9296\n",
      "Epoch 80/500, Loss: 0.07877666503190994, Test Accuracy: 0.9296\n",
      "Epoch 81/500, Loss: 0.08432351052761078, Test Accuracy: 0.9296\n",
      "Epoch 82/500, Loss: 0.09623581171035767, Test Accuracy: 0.9155\n",
      "Epoch 83/500, Loss: 0.08556783944368362, Test Accuracy: 0.9155\n",
      "Epoch 84/500, Loss: 0.07627429068088531, Test Accuracy: 0.9296\n",
      "Epoch 85/500, Loss: 0.08284293115139008, Test Accuracy: 0.9155\n",
      "Epoch 86/500, Loss: 0.08833947032690048, Test Accuracy: 0.9155\n",
      "Epoch 87/500, Loss: 0.08672921359539032, Test Accuracy: 0.9155\n",
      "Epoch 88/500, Loss: 0.08770626783370972, Test Accuracy: 0.9155\n",
      "Epoch 89/500, Loss: 0.07442738115787506, Test Accuracy: 0.9155\n",
      "Epoch 90/500, Loss: 0.07304953783750534, Test Accuracy: 0.9155\n",
      "Epoch 91/500, Loss: 0.0709645077586174, Test Accuracy: 0.9155\n",
      "Epoch 92/500, Loss: 0.08071579039096832, Test Accuracy: 0.9014\n",
      "Epoch 93/500, Loss: 0.08117907494306564, Test Accuracy: 0.9014\n",
      "Epoch 94/500, Loss: 0.07364964485168457, Test Accuracy: 0.9014\n",
      "Epoch 95/500, Loss: 0.06881359964609146, Test Accuracy: 0.9014\n",
      "Epoch 96/500, Loss: 0.08356083929538727, Test Accuracy: 0.9014\n",
      "Epoch 97/500, Loss: 0.05912194028496742, Test Accuracy: 0.9014\n",
      "Epoch 98/500, Loss: 0.07376080751419067, Test Accuracy: 0.9014\n",
      "Epoch 99/500, Loss: 0.08132794499397278, Test Accuracy: 0.9014\n",
      "Epoch 100/500, Loss: 0.07241189479827881, Test Accuracy: 0.9014\n",
      "Epoch 101/500, Loss: 0.06385096907615662, Test Accuracy: 0.9014\n",
      "Epoch 102/500, Loss: 0.05741194263100624, Test Accuracy: 0.9014\n",
      "Epoch 103/500, Loss: 0.07402420789003372, Test Accuracy: 0.9014\n",
      "Epoch 104/500, Loss: 0.06373237073421478, Test Accuracy: 0.9014\n",
      "Epoch 105/500, Loss: 0.06552507728338242, Test Accuracy: 0.9014\n",
      "Epoch 106/500, Loss: 0.06887855380773544, Test Accuracy: 0.9014\n",
      "Epoch 107/500, Loss: 0.055743731558322906, Test Accuracy: 0.9014\n",
      "Epoch 108/500, Loss: 0.05315990373492241, Test Accuracy: 0.9014\n",
      "Epoch 109/500, Loss: 0.061742641031742096, Test Accuracy: 0.9014\n",
      "Epoch 110/500, Loss: 0.060848087072372437, Test Accuracy: 0.9155\n",
      "Epoch 111/500, Loss: 0.059083305299282074, Test Accuracy: 0.9155\n",
      "Epoch 112/500, Loss: 0.06111341714859009, Test Accuracy: 0.9014\n",
      "Epoch 113/500, Loss: 0.0672762393951416, Test Accuracy: 0.9014\n",
      "Epoch 114/500, Loss: 0.05859190598130226, Test Accuracy: 0.9014\n",
      "Epoch 115/500, Loss: 0.06632959842681885, Test Accuracy: 0.9014\n",
      "Epoch 116/500, Loss: 0.06305520236492157, Test Accuracy: 0.9014\n",
      "Epoch 117/500, Loss: 0.06194377318024635, Test Accuracy: 0.9014\n",
      "Epoch 118/500, Loss: 0.05434533581137657, Test Accuracy: 0.9014\n",
      "Epoch 119/500, Loss: 0.06313251703977585, Test Accuracy: 0.9014\n",
      "Epoch 120/500, Loss: 0.04394400119781494, Test Accuracy: 0.9014\n",
      "Epoch 121/500, Loss: 0.0561281181871891, Test Accuracy: 0.9014\n",
      "Epoch 122/500, Loss: 0.05262163281440735, Test Accuracy: 0.9014\n",
      "Epoch 123/500, Loss: 0.051904596388339996, Test Accuracy: 0.9014\n",
      "Epoch 124/500, Loss: 0.06526268273591995, Test Accuracy: 0.9014\n",
      "Epoch 125/500, Loss: 0.03869498148560524, Test Accuracy: 0.9014\n",
      "Epoch 126/500, Loss: 0.055347003042697906, Test Accuracy: 0.9014\n",
      "Epoch 127/500, Loss: 0.05459095165133476, Test Accuracy: 0.9014\n",
      "Epoch 128/500, Loss: 0.035651396960020065, Test Accuracy: 0.9014\n",
      "Epoch 129/500, Loss: 0.053220730274915695, Test Accuracy: 0.9014\n",
      "Epoch 130/500, Loss: 0.044480957090854645, Test Accuracy: 0.9014\n",
      "Early stopping triggered\n",
      "Best Epoch: 30\n",
      "Confusion Matrix:\n",
      " [[30  5]\n",
      " [ 2 34]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.94      0.86      0.90        35\n",
      "     Class 1       0.87      0.94      0.91        36\n",
      "\n",
      "    accuracy                           0.90        71\n",
      "   macro avg       0.90      0.90      0.90        71\n",
      "weighted avg       0.90      0.90      0.90        71\n",
      "\n",
      "Best Test Accuracy: 0.9014\n",
      "Fold 4\n",
      "Epoch 1/500, Loss: 0.8996190428733826, Test Accuracy: 0.3662\n",
      "Epoch 2/500, Loss: 0.8542740941047668, Test Accuracy: 0.4507\n",
      "Epoch 3/500, Loss: 0.7879969477653503, Test Accuracy: 0.5070\n",
      "Epoch 4/500, Loss: 0.7556794881820679, Test Accuracy: 0.5915\n",
      "Epoch 5/500, Loss: 0.7195439338684082, Test Accuracy: 0.6620\n",
      "Epoch 6/500, Loss: 0.6818957924842834, Test Accuracy: 0.7183\n",
      "Epoch 7/500, Loss: 0.6389071345329285, Test Accuracy: 0.7746\n",
      "Epoch 8/500, Loss: 0.5905071496963501, Test Accuracy: 0.8028\n",
      "Epoch 9/500, Loss: 0.5793343186378479, Test Accuracy: 0.8451\n",
      "Epoch 10/500, Loss: 0.5389823913574219, Test Accuracy: 0.8732\n",
      "Epoch 11/500, Loss: 0.5036967396736145, Test Accuracy: 0.8873\n",
      "Epoch 12/500, Loss: 0.47638845443725586, Test Accuracy: 0.9014\n",
      "Epoch 13/500, Loss: 0.45898735523223877, Test Accuracy: 0.9014\n",
      "Epoch 14/500, Loss: 0.43390414118766785, Test Accuracy: 0.9014\n",
      "Epoch 15/500, Loss: 0.4046390950679779, Test Accuracy: 0.9155\n",
      "Epoch 16/500, Loss: 0.39492130279541016, Test Accuracy: 0.9296\n",
      "Epoch 17/500, Loss: 0.3844367265701294, Test Accuracy: 0.9296\n",
      "Epoch 18/500, Loss: 0.35851019620895386, Test Accuracy: 0.9296\n",
      "Epoch 19/500, Loss: 0.3436623811721802, Test Accuracy: 0.9296\n",
      "Epoch 20/500, Loss: 0.33534759283065796, Test Accuracy: 0.9296\n",
      "Epoch 21/500, Loss: 0.31619057059288025, Test Accuracy: 0.9296\n",
      "Epoch 22/500, Loss: 0.3051412105560303, Test Accuracy: 0.9296\n",
      "Epoch 23/500, Loss: 0.2909143567085266, Test Accuracy: 0.9296\n",
      "Epoch 24/500, Loss: 0.273809015750885, Test Accuracy: 0.9296\n",
      "Epoch 25/500, Loss: 0.27284178137779236, Test Accuracy: 0.9296\n",
      "Epoch 26/500, Loss: 0.2579196095466614, Test Accuracy: 0.9296\n",
      "Epoch 27/500, Loss: 0.25164994597435, Test Accuracy: 0.9296\n",
      "Epoch 28/500, Loss: 0.239926278591156, Test Accuracy: 0.9296\n",
      "Epoch 29/500, Loss: 0.23276427388191223, Test Accuracy: 0.9296\n",
      "Epoch 30/500, Loss: 0.21243484318256378, Test Accuracy: 0.9437\n",
      "Epoch 31/500, Loss: 0.22009041905403137, Test Accuracy: 0.9437\n",
      "Epoch 32/500, Loss: 0.20727549493312836, Test Accuracy: 0.9437\n",
      "Epoch 33/500, Loss: 0.20749486982822418, Test Accuracy: 0.9437\n",
      "Epoch 34/500, Loss: 0.18244576454162598, Test Accuracy: 0.9437\n",
      "Epoch 35/500, Loss: 0.19521218538284302, Test Accuracy: 0.9437\n",
      "Epoch 36/500, Loss: 0.19292554259300232, Test Accuracy: 0.9437\n",
      "Epoch 37/500, Loss: 0.18901389837265015, Test Accuracy: 0.9437\n",
      "Epoch 38/500, Loss: 0.17680835723876953, Test Accuracy: 0.9437\n",
      "Epoch 39/500, Loss: 0.1851566582918167, Test Accuracy: 0.9437\n",
      "Epoch 40/500, Loss: 0.169840008020401, Test Accuracy: 0.9437\n",
      "Epoch 41/500, Loss: 0.15987233817577362, Test Accuracy: 0.9437\n",
      "Epoch 42/500, Loss: 0.17319625616073608, Test Accuracy: 0.9437\n",
      "Epoch 43/500, Loss: 0.15036341547966003, Test Accuracy: 0.9437\n",
      "Epoch 44/500, Loss: 0.15693339705467224, Test Accuracy: 0.9437\n",
      "Epoch 45/500, Loss: 0.14235946536064148, Test Accuracy: 0.9437\n",
      "Epoch 46/500, Loss: 0.14798356592655182, Test Accuracy: 0.9437\n",
      "Epoch 47/500, Loss: 0.1421334594488144, Test Accuracy: 0.9437\n",
      "Epoch 48/500, Loss: 0.13737905025482178, Test Accuracy: 0.9437\n",
      "Epoch 49/500, Loss: 0.1274515986442566, Test Accuracy: 0.9437\n",
      "Epoch 50/500, Loss: 0.12777355313301086, Test Accuracy: 0.9437\n",
      "Epoch 51/500, Loss: 0.12334766238927841, Test Accuracy: 0.9437\n",
      "Epoch 52/500, Loss: 0.12368512153625488, Test Accuracy: 0.9437\n",
      "Epoch 53/500, Loss: 0.14112284779548645, Test Accuracy: 0.9437\n",
      "Epoch 54/500, Loss: 0.1165219098329544, Test Accuracy: 0.9437\n",
      "Epoch 55/500, Loss: 0.1119401678442955, Test Accuracy: 0.9437\n",
      "Epoch 56/500, Loss: 0.1176336333155632, Test Accuracy: 0.9437\n",
      "Epoch 57/500, Loss: 0.11234351247549057, Test Accuracy: 0.9577\n",
      "Epoch 58/500, Loss: 0.11335759609937668, Test Accuracy: 0.9577\n",
      "Epoch 59/500, Loss: 0.10544654726982117, Test Accuracy: 0.9577\n",
      "Epoch 60/500, Loss: 0.09701725095510483, Test Accuracy: 0.9577\n",
      "Epoch 61/500, Loss: 0.10074789077043533, Test Accuracy: 0.9577\n",
      "Epoch 62/500, Loss: 0.10980354994535446, Test Accuracy: 0.9577\n",
      "Epoch 63/500, Loss: 0.10035891830921173, Test Accuracy: 0.9577\n",
      "Epoch 64/500, Loss: 0.09903668612241745, Test Accuracy: 0.9577\n",
      "Epoch 65/500, Loss: 0.08875401318073273, Test Accuracy: 0.9577\n",
      "Epoch 66/500, Loss: 0.09828947484493256, Test Accuracy: 0.9577\n",
      "Epoch 67/500, Loss: 0.09252214431762695, Test Accuracy: 0.9577\n",
      "Epoch 68/500, Loss: 0.09139659255743027, Test Accuracy: 0.9577\n",
      "Epoch 69/500, Loss: 0.09589340537786484, Test Accuracy: 0.9577\n",
      "Epoch 70/500, Loss: 0.10465497523546219, Test Accuracy: 0.9577\n",
      "Epoch 71/500, Loss: 0.08692027628421783, Test Accuracy: 0.9577\n",
      "Epoch 72/500, Loss: 0.08386358618736267, Test Accuracy: 0.9577\n",
      "Epoch 73/500, Loss: 0.09199471771717072, Test Accuracy: 0.9577\n",
      "Epoch 74/500, Loss: 0.08021591603755951, Test Accuracy: 0.9437\n",
      "Epoch 75/500, Loss: 0.08116711676120758, Test Accuracy: 0.9437\n",
      "Epoch 76/500, Loss: 0.0838780552148819, Test Accuracy: 0.9437\n",
      "Epoch 77/500, Loss: 0.07961433380842209, Test Accuracy: 0.9437\n",
      "Epoch 78/500, Loss: 0.08976603299379349, Test Accuracy: 0.9437\n",
      "Epoch 79/500, Loss: 0.0834333673119545, Test Accuracy: 0.9437\n",
      "Epoch 80/500, Loss: 0.07059624046087265, Test Accuracy: 0.9437\n",
      "Epoch 81/500, Loss: 0.0801786333322525, Test Accuracy: 0.9437\n",
      "Epoch 82/500, Loss: 0.08330130577087402, Test Accuracy: 0.9437\n",
      "Epoch 83/500, Loss: 0.07042871415615082, Test Accuracy: 0.9437\n",
      "Epoch 84/500, Loss: 0.07285275310277939, Test Accuracy: 0.9437\n",
      "Epoch 85/500, Loss: 0.06540712714195251, Test Accuracy: 0.9437\n",
      "Epoch 86/500, Loss: 0.08070966601371765, Test Accuracy: 0.9437\n",
      "Epoch 87/500, Loss: 0.06696580350399017, Test Accuracy: 0.9437\n",
      "Epoch 88/500, Loss: 0.06366045027971268, Test Accuracy: 0.9437\n",
      "Epoch 89/500, Loss: 0.08299143612384796, Test Accuracy: 0.9437\n",
      "Epoch 90/500, Loss: 0.061527594923973083, Test Accuracy: 0.9437\n",
      "Epoch 91/500, Loss: 0.07478871941566467, Test Accuracy: 0.9437\n",
      "Epoch 92/500, Loss: 0.07089477777481079, Test Accuracy: 0.9437\n",
      "Epoch 93/500, Loss: 0.07347898930311203, Test Accuracy: 0.9437\n",
      "Epoch 94/500, Loss: 0.06123434007167816, Test Accuracy: 0.9437\n",
      "Epoch 95/500, Loss: 0.06668642908334732, Test Accuracy: 0.9296\n",
      "Epoch 96/500, Loss: 0.05987244099378586, Test Accuracy: 0.9296\n",
      "Epoch 97/500, Loss: 0.05339575186371803, Test Accuracy: 0.9296\n",
      "Epoch 98/500, Loss: 0.06461301445960999, Test Accuracy: 0.9437\n",
      "Epoch 99/500, Loss: 0.06720288097858429, Test Accuracy: 0.9437\n",
      "Epoch 100/500, Loss: 0.05544624477624893, Test Accuracy: 0.9437\n",
      "Epoch 101/500, Loss: 0.0546230785548687, Test Accuracy: 0.9437\n",
      "Epoch 102/500, Loss: 0.05608309060335159, Test Accuracy: 0.9437\n",
      "Epoch 103/500, Loss: 0.04531608521938324, Test Accuracy: 0.9437\n",
      "Epoch 104/500, Loss: 0.05336804315447807, Test Accuracy: 0.9437\n",
      "Epoch 105/500, Loss: 0.06335953623056412, Test Accuracy: 0.9437\n",
      "Epoch 106/500, Loss: 0.05669431388378143, Test Accuracy: 0.9437\n",
      "Epoch 107/500, Loss: 0.0590067096054554, Test Accuracy: 0.9437\n",
      "Epoch 108/500, Loss: 0.050625987350940704, Test Accuracy: 0.9437\n",
      "Epoch 109/500, Loss: 0.060841359198093414, Test Accuracy: 0.9296\n",
      "Epoch 110/500, Loss: 0.05627595633268356, Test Accuracy: 0.9296\n",
      "Epoch 111/500, Loss: 0.05206676200032234, Test Accuracy: 0.9296\n",
      "Epoch 112/500, Loss: 0.05301032215356827, Test Accuracy: 0.9296\n",
      "Epoch 113/500, Loss: 0.053562749177217484, Test Accuracy: 0.9296\n",
      "Epoch 114/500, Loss: 0.04404132440686226, Test Accuracy: 0.9296\n",
      "Epoch 115/500, Loss: 0.04712532088160515, Test Accuracy: 0.9296\n",
      "Epoch 116/500, Loss: 0.03731236979365349, Test Accuracy: 0.9296\n",
      "Epoch 117/500, Loss: 0.04448437690734863, Test Accuracy: 0.9296\n",
      "Epoch 118/500, Loss: 0.04014904052019119, Test Accuracy: 0.9296\n",
      "Epoch 119/500, Loss: 0.052344806492328644, Test Accuracy: 0.9296\n",
      "Epoch 120/500, Loss: 0.046669382601976395, Test Accuracy: 0.9296\n",
      "Epoch 121/500, Loss: 0.03728461638092995, Test Accuracy: 0.9296\n",
      "Epoch 122/500, Loss: 0.04829717054963112, Test Accuracy: 0.9296\n",
      "Epoch 123/500, Loss: 0.036673642694950104, Test Accuracy: 0.9296\n",
      "Epoch 124/500, Loss: 0.03953027352690697, Test Accuracy: 0.9296\n",
      "Epoch 125/500, Loss: 0.045900292694568634, Test Accuracy: 0.9296\n",
      "Epoch 126/500, Loss: 0.04512329027056694, Test Accuracy: 0.9296\n",
      "Epoch 127/500, Loss: 0.03646862879395485, Test Accuracy: 0.9296\n",
      "Epoch 128/500, Loss: 0.03779476508498192, Test Accuracy: 0.9296\n",
      "Epoch 129/500, Loss: 0.03802371397614479, Test Accuracy: 0.9437\n",
      "Epoch 130/500, Loss: 0.0366084948182106, Test Accuracy: 0.9437\n",
      "Epoch 131/500, Loss: 0.0351736955344677, Test Accuracy: 0.9437\n",
      "Epoch 132/500, Loss: 0.03307804465293884, Test Accuracy: 0.9437\n",
      "Epoch 133/500, Loss: 0.05069899559020996, Test Accuracy: 0.9437\n",
      "Epoch 134/500, Loss: 0.04054279625415802, Test Accuracy: 0.9296\n",
      "Epoch 135/500, Loss: 0.03777299448847771, Test Accuracy: 0.9296\n",
      "Epoch 136/500, Loss: 0.03928384929895401, Test Accuracy: 0.9296\n",
      "Epoch 137/500, Loss: 0.03783000633120537, Test Accuracy: 0.9296\n",
      "Epoch 138/500, Loss: 0.03896290808916092, Test Accuracy: 0.9296\n",
      "Epoch 139/500, Loss: 0.03431744873523712, Test Accuracy: 0.9155\n",
      "Epoch 140/500, Loss: 0.03744985908269882, Test Accuracy: 0.9155\n",
      "Epoch 141/500, Loss: 0.03479913994669914, Test Accuracy: 0.9155\n",
      "Epoch 142/500, Loss: 0.03653557226061821, Test Accuracy: 0.9014\n",
      "Epoch 143/500, Loss: 0.04033372923731804, Test Accuracy: 0.9014\n",
      "Epoch 144/500, Loss: 0.03695261850953102, Test Accuracy: 0.9014\n",
      "Epoch 145/500, Loss: 0.04994530975818634, Test Accuracy: 0.9155\n",
      "Epoch 146/500, Loss: 0.039857637137174606, Test Accuracy: 0.9296\n",
      "Epoch 147/500, Loss: 0.032661233097314835, Test Accuracy: 0.9296\n",
      "Epoch 148/500, Loss: 0.037083979696035385, Test Accuracy: 0.9296\n",
      "Epoch 149/500, Loss: 0.04055583104491234, Test Accuracy: 0.9296\n",
      "Epoch 150/500, Loss: 0.034508269280195236, Test Accuracy: 0.9296\n",
      "Epoch 151/500, Loss: 0.03903087228536606, Test Accuracy: 0.9155\n",
      "Epoch 152/500, Loss: 0.03001348488032818, Test Accuracy: 0.9155\n",
      "Epoch 153/500, Loss: 0.027346516028046608, Test Accuracy: 0.9155\n",
      "Epoch 154/500, Loss: 0.03154831379652023, Test Accuracy: 0.9155\n",
      "Epoch 155/500, Loss: 0.04283703863620758, Test Accuracy: 0.9155\n",
      "Epoch 156/500, Loss: 0.030887549743056297, Test Accuracy: 0.9155\n",
      "Epoch 157/500, Loss: 0.030398136004805565, Test Accuracy: 0.9155\n",
      "Early stopping triggered\n",
      "Best Epoch: 57\n",
      "Confusion Matrix:\n",
      " [[30  4]\n",
      " [ 2 35]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.94      0.88      0.91        34\n",
      "     Class 1       0.90      0.95      0.92        37\n",
      "\n",
      "    accuracy                           0.92        71\n",
      "   macro avg       0.92      0.91      0.92        71\n",
      "weighted avg       0.92      0.92      0.92        71\n",
      "\n",
      "Best Test Accuracy: 0.9155\n",
      "Fold 5\n",
      "Epoch 1/500, Loss: 0.7773906588554382, Test Accuracy: 0.6286\n",
      "Epoch 2/500, Loss: 0.6875755190849304, Test Accuracy: 0.6857\n",
      "Epoch 3/500, Loss: 0.6573741436004639, Test Accuracy: 0.7429\n",
      "Epoch 4/500, Loss: 0.6048563122749329, Test Accuracy: 0.7857\n",
      "Epoch 5/500, Loss: 0.556283712387085, Test Accuracy: 0.8286\n",
      "Epoch 6/500, Loss: 0.514239490032196, Test Accuracy: 0.8429\n",
      "Epoch 7/500, Loss: 0.4685273766517639, Test Accuracy: 0.8857\n",
      "Epoch 8/500, Loss: 0.45209598541259766, Test Accuracy: 0.9286\n",
      "Epoch 9/500, Loss: 0.42023909091949463, Test Accuracy: 0.9429\n",
      "Epoch 10/500, Loss: 0.3923901319503784, Test Accuracy: 0.9429\n",
      "Epoch 11/500, Loss: 0.36640292406082153, Test Accuracy: 0.9714\n",
      "Epoch 12/500, Loss: 0.3513750731945038, Test Accuracy: 1.0000\n",
      "Epoch 13/500, Loss: 0.31346189975738525, Test Accuracy: 1.0000\n",
      "Epoch 14/500, Loss: 0.29359906911849976, Test Accuracy: 1.0000\n",
      "Epoch 15/500, Loss: 0.2877363860607147, Test Accuracy: 1.0000\n",
      "Epoch 16/500, Loss: 0.2618084251880646, Test Accuracy: 1.0000\n",
      "Epoch 17/500, Loss: 0.25328898429870605, Test Accuracy: 1.0000\n",
      "Epoch 18/500, Loss: 0.2416456937789917, Test Accuracy: 1.0000\n",
      "Epoch 19/500, Loss: 0.2243313491344452, Test Accuracy: 1.0000\n",
      "Epoch 20/500, Loss: 0.21670174598693848, Test Accuracy: 1.0000\n",
      "Epoch 21/500, Loss: 0.21605093777179718, Test Accuracy: 1.0000\n",
      "Epoch 22/500, Loss: 0.19177569448947906, Test Accuracy: 1.0000\n",
      "Epoch 23/500, Loss: 0.1789006143808365, Test Accuracy: 1.0000\n",
      "Epoch 24/500, Loss: 0.16593530774116516, Test Accuracy: 1.0000\n",
      "Epoch 25/500, Loss: 0.16555817425251007, Test Accuracy: 1.0000\n",
      "Epoch 26/500, Loss: 0.157098188996315, Test Accuracy: 1.0000\n",
      "Epoch 27/500, Loss: 0.15230393409729004, Test Accuracy: 1.0000\n",
      "Epoch 28/500, Loss: 0.145896777510643, Test Accuracy: 0.9857\n",
      "Epoch 29/500, Loss: 0.13478712737560272, Test Accuracy: 0.9857\n",
      "Epoch 30/500, Loss: 0.13454079627990723, Test Accuracy: 0.9857\n",
      "Epoch 31/500, Loss: 0.13325724005699158, Test Accuracy: 0.9857\n",
      "Epoch 32/500, Loss: 0.12419728934764862, Test Accuracy: 0.9857\n",
      "Epoch 33/500, Loss: 0.11688285320997238, Test Accuracy: 0.9857\n",
      "Epoch 34/500, Loss: 0.10812113434076309, Test Accuracy: 0.9857\n",
      "Epoch 35/500, Loss: 0.10864020884037018, Test Accuracy: 0.9857\n",
      "Epoch 36/500, Loss: 0.10478822886943817, Test Accuracy: 0.9857\n",
      "Epoch 37/500, Loss: 0.0986519381403923, Test Accuracy: 0.9857\n",
      "Epoch 38/500, Loss: 0.11006708443164825, Test Accuracy: 0.9857\n",
      "Epoch 39/500, Loss: 0.10013213753700256, Test Accuracy: 0.9857\n",
      "Epoch 40/500, Loss: 0.0926133394241333, Test Accuracy: 0.9857\n",
      "Epoch 41/500, Loss: 0.08435987681150436, Test Accuracy: 0.9857\n",
      "Epoch 42/500, Loss: 0.08191197365522385, Test Accuracy: 0.9857\n",
      "Epoch 43/500, Loss: 0.07497642934322357, Test Accuracy: 0.9857\n",
      "Epoch 44/500, Loss: 0.07661199569702148, Test Accuracy: 0.9857\n",
      "Epoch 45/500, Loss: 0.08269956707954407, Test Accuracy: 0.9857\n",
      "Epoch 46/500, Loss: 0.07413265109062195, Test Accuracy: 0.9857\n",
      "Epoch 47/500, Loss: 0.07665206491947174, Test Accuracy: 0.9857\n",
      "Epoch 48/500, Loss: 0.06485572457313538, Test Accuracy: 0.9857\n",
      "Epoch 49/500, Loss: 0.07624240964651108, Test Accuracy: 0.9857\n",
      "Epoch 50/500, Loss: 0.07250473648309708, Test Accuracy: 0.9857\n",
      "Epoch 51/500, Loss: 0.07223694771528244, Test Accuracy: 0.9857\n",
      "Epoch 52/500, Loss: 0.07287859916687012, Test Accuracy: 0.9857\n",
      "Epoch 53/500, Loss: 0.06662491708993912, Test Accuracy: 0.9857\n",
      "Epoch 54/500, Loss: 0.06626590341329575, Test Accuracy: 0.9857\n",
      "Epoch 55/500, Loss: 0.08196033537387848, Test Accuracy: 0.9857\n",
      "Epoch 56/500, Loss: 0.0742819681763649, Test Accuracy: 0.9857\n",
      "Epoch 57/500, Loss: 0.08012659847736359, Test Accuracy: 0.9857\n",
      "Epoch 58/500, Loss: 0.05360084027051926, Test Accuracy: 0.9857\n",
      "Epoch 59/500, Loss: 0.0533476397395134, Test Accuracy: 0.9857\n",
      "Epoch 60/500, Loss: 0.06808873265981674, Test Accuracy: 0.9857\n",
      "Epoch 61/500, Loss: 0.047772906720638275, Test Accuracy: 0.9857\n",
      "Epoch 62/500, Loss: 0.05725428834557533, Test Accuracy: 0.9857\n",
      "Epoch 63/500, Loss: 0.05348842963576317, Test Accuracy: 0.9857\n",
      "Epoch 64/500, Loss: 0.054341938346624374, Test Accuracy: 0.9857\n",
      "Epoch 65/500, Loss: 0.049220968037843704, Test Accuracy: 0.9857\n",
      "Epoch 66/500, Loss: 0.06058774143457413, Test Accuracy: 0.9857\n",
      "Epoch 67/500, Loss: 0.04315073415637016, Test Accuracy: 0.9857\n",
      "Epoch 68/500, Loss: 0.05223575606942177, Test Accuracy: 0.9857\n",
      "Epoch 69/500, Loss: 0.05826130509376526, Test Accuracy: 0.9857\n",
      "Epoch 70/500, Loss: 0.05827295407652855, Test Accuracy: 0.9857\n",
      "Epoch 71/500, Loss: 0.04465704411268234, Test Accuracy: 0.9857\n",
      "Epoch 72/500, Loss: 0.042885541915893555, Test Accuracy: 0.9857\n",
      "Epoch 73/500, Loss: 0.03982241451740265, Test Accuracy: 0.9857\n",
      "Epoch 74/500, Loss: 0.059530314058065414, Test Accuracy: 0.9857\n",
      "Epoch 75/500, Loss: 0.04395153373479843, Test Accuracy: 0.9857\n",
      "Epoch 76/500, Loss: 0.04470629617571831, Test Accuracy: 0.9857\n",
      "Epoch 77/500, Loss: 0.04817848652601242, Test Accuracy: 0.9857\n",
      "Epoch 78/500, Loss: 0.04723060876131058, Test Accuracy: 0.9857\n",
      "Epoch 79/500, Loss: 0.039193056523799896, Test Accuracy: 0.9857\n",
      "Epoch 80/500, Loss: 0.04391038790345192, Test Accuracy: 0.9857\n",
      "Epoch 81/500, Loss: 0.03916516155004501, Test Accuracy: 0.9857\n",
      "Epoch 82/500, Loss: 0.04971257597208023, Test Accuracy: 0.9857\n",
      "Epoch 83/500, Loss: 0.04000253602862358, Test Accuracy: 0.9714\n",
      "Epoch 84/500, Loss: 0.05355587601661682, Test Accuracy: 0.9714\n",
      "Epoch 85/500, Loss: 0.044040244072675705, Test Accuracy: 0.9714\n",
      "Epoch 86/500, Loss: 0.04223913326859474, Test Accuracy: 0.9714\n",
      "Epoch 87/500, Loss: 0.05313458293676376, Test Accuracy: 0.9714\n",
      "Epoch 88/500, Loss: 0.03512848541140556, Test Accuracy: 0.9714\n",
      "Epoch 89/500, Loss: 0.03697405755519867, Test Accuracy: 0.9714\n",
      "Epoch 90/500, Loss: 0.03536653891205788, Test Accuracy: 0.9714\n",
      "Epoch 91/500, Loss: 0.048582304269075394, Test Accuracy: 0.9714\n",
      "Epoch 92/500, Loss: 0.030695445835590363, Test Accuracy: 0.9714\n",
      "Epoch 93/500, Loss: 0.03656424582004547, Test Accuracy: 0.9714\n",
      "Epoch 94/500, Loss: 0.028970154002308846, Test Accuracy: 0.9714\n",
      "Epoch 95/500, Loss: 0.03510838747024536, Test Accuracy: 0.9714\n",
      "Epoch 96/500, Loss: 0.035916682332754135, Test Accuracy: 0.9714\n",
      "Epoch 97/500, Loss: 0.0313640832901001, Test Accuracy: 0.9714\n",
      "Epoch 98/500, Loss: 0.030784279108047485, Test Accuracy: 0.9714\n",
      "Epoch 99/500, Loss: 0.03497634083032608, Test Accuracy: 0.9714\n",
      "Epoch 100/500, Loss: 0.0272016990929842, Test Accuracy: 0.9714\n",
      "Epoch 101/500, Loss: 0.04080634564161301, Test Accuracy: 0.9714\n",
      "Epoch 102/500, Loss: 0.028343014419078827, Test Accuracy: 0.9714\n",
      "Epoch 103/500, Loss: 0.034075282514095306, Test Accuracy: 0.9714\n",
      "Epoch 104/500, Loss: 0.02852495200932026, Test Accuracy: 0.9714\n",
      "Epoch 105/500, Loss: 0.02965117059648037, Test Accuracy: 0.9714\n",
      "Epoch 106/500, Loss: 0.03407187759876251, Test Accuracy: 0.9714\n",
      "Epoch 107/500, Loss: 0.026401806622743607, Test Accuracy: 0.9714\n",
      "Epoch 108/500, Loss: 0.027785129845142365, Test Accuracy: 0.9714\n",
      "Epoch 109/500, Loss: 0.025988396257162094, Test Accuracy: 0.9714\n",
      "Epoch 110/500, Loss: 0.037368882447481155, Test Accuracy: 0.9714\n",
      "Epoch 111/500, Loss: 0.022317353636026382, Test Accuracy: 0.9714\n",
      "Epoch 112/500, Loss: 0.023878924548625946, Test Accuracy: 0.9714\n",
      "Early stopping triggered\n",
      "Best Epoch: 12\n",
      "Confusion Matrix:\n",
      " [[39  0]\n",
      " [ 2 29]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.95      1.00      0.97        39\n",
      "     Class 1       1.00      0.94      0.97        31\n",
      "\n",
      "    accuracy                           0.97        70\n",
      "   macro avg       0.98      0.97      0.97        70\n",
      "weighted avg       0.97      0.97      0.97        70\n",
      "\n",
      "Best Test Accuracy: 0.9714\n",
      "Fold 6\n",
      "Epoch 1/500, Loss: 1.1936551332473755, Test Accuracy: 0.0429\n",
      "Epoch 2/500, Loss: 1.1025499105453491, Test Accuracy: 0.0714\n",
      "Epoch 3/500, Loss: 1.0508402585983276, Test Accuracy: 0.1143\n",
      "Epoch 4/500, Loss: 0.9512991905212402, Test Accuracy: 0.1714\n",
      "Epoch 5/500, Loss: 0.9061994552612305, Test Accuracy: 0.2143\n",
      "Epoch 6/500, Loss: 0.8371927738189697, Test Accuracy: 0.3429\n",
      "Epoch 7/500, Loss: 0.77430260181427, Test Accuracy: 0.4571\n",
      "Epoch 8/500, Loss: 0.7232018113136292, Test Accuracy: 0.5714\n",
      "Epoch 9/500, Loss: 0.6598997712135315, Test Accuracy: 0.7143\n",
      "Epoch 10/500, Loss: 0.6209599375724792, Test Accuracy: 0.8714\n",
      "Epoch 11/500, Loss: 0.5809584856033325, Test Accuracy: 0.9000\n",
      "Epoch 12/500, Loss: 0.5485203862190247, Test Accuracy: 0.9000\n",
      "Epoch 13/500, Loss: 0.49895384907722473, Test Accuracy: 0.9571\n",
      "Epoch 14/500, Loss: 0.4693462550640106, Test Accuracy: 0.9571\n",
      "Epoch 15/500, Loss: 0.44137197732925415, Test Accuracy: 0.9714\n",
      "Epoch 16/500, Loss: 0.4208073318004608, Test Accuracy: 0.9714\n",
      "Epoch 17/500, Loss: 0.3872852921485901, Test Accuracy: 0.9857\n",
      "Epoch 18/500, Loss: 0.3643687069416046, Test Accuracy: 0.9857\n",
      "Epoch 19/500, Loss: 0.3430247902870178, Test Accuracy: 1.0000\n",
      "Epoch 20/500, Loss: 0.32146981358528137, Test Accuracy: 1.0000\n",
      "Epoch 21/500, Loss: 0.3023134768009186, Test Accuracy: 1.0000\n",
      "Epoch 22/500, Loss: 0.28426453471183777, Test Accuracy: 1.0000\n",
      "Epoch 23/500, Loss: 0.2841030955314636, Test Accuracy: 1.0000\n",
      "Epoch 24/500, Loss: 0.2624976634979248, Test Accuracy: 1.0000\n",
      "Epoch 25/500, Loss: 0.2511131763458252, Test Accuracy: 1.0000\n",
      "Epoch 26/500, Loss: 0.23670782148838043, Test Accuracy: 1.0000\n",
      "Epoch 27/500, Loss: 0.22829562425613403, Test Accuracy: 1.0000\n",
      "Epoch 28/500, Loss: 0.21543578803539276, Test Accuracy: 1.0000\n",
      "Epoch 29/500, Loss: 0.20497454702854156, Test Accuracy: 1.0000\n",
      "Epoch 30/500, Loss: 0.19777260720729828, Test Accuracy: 1.0000\n",
      "Epoch 31/500, Loss: 0.1825500726699829, Test Accuracy: 1.0000\n",
      "Epoch 32/500, Loss: 0.1778009682893753, Test Accuracy: 1.0000\n",
      "Epoch 33/500, Loss: 0.16219234466552734, Test Accuracy: 1.0000\n",
      "Epoch 34/500, Loss: 0.1599702388048172, Test Accuracy: 1.0000\n",
      "Epoch 35/500, Loss: 0.15447375178337097, Test Accuracy: 1.0000\n",
      "Epoch 36/500, Loss: 0.1482400894165039, Test Accuracy: 1.0000\n",
      "Epoch 37/500, Loss: 0.14386044442653656, Test Accuracy: 1.0000\n",
      "Epoch 38/500, Loss: 0.14140096306800842, Test Accuracy: 1.0000\n",
      "Epoch 39/500, Loss: 0.1268126219511032, Test Accuracy: 1.0000\n",
      "Epoch 40/500, Loss: 0.12396740913391113, Test Accuracy: 1.0000\n",
      "Epoch 41/500, Loss: 0.12742574512958527, Test Accuracy: 1.0000\n",
      "Epoch 42/500, Loss: 0.11595173180103302, Test Accuracy: 1.0000\n",
      "Epoch 43/500, Loss: 0.11545707285404205, Test Accuracy: 1.0000\n",
      "Epoch 44/500, Loss: 0.1027136743068695, Test Accuracy: 1.0000\n",
      "Epoch 45/500, Loss: 0.11060831695795059, Test Accuracy: 1.0000\n",
      "Epoch 46/500, Loss: 0.10341254621744156, Test Accuracy: 1.0000\n",
      "Epoch 47/500, Loss: 0.1023947075009346, Test Accuracy: 1.0000\n",
      "Epoch 48/500, Loss: 0.10469154268503189, Test Accuracy: 1.0000\n",
      "Epoch 49/500, Loss: 0.09346549957990646, Test Accuracy: 1.0000\n",
      "Epoch 50/500, Loss: 0.08935876935720444, Test Accuracy: 1.0000\n",
      "Epoch 51/500, Loss: 0.08717244118452072, Test Accuracy: 1.0000\n",
      "Epoch 52/500, Loss: 0.08789800107479095, Test Accuracy: 1.0000\n",
      "Epoch 53/500, Loss: 0.08373527973890305, Test Accuracy: 1.0000\n",
      "Epoch 54/500, Loss: 0.07707081735134125, Test Accuracy: 1.0000\n",
      "Epoch 55/500, Loss: 0.07911459356546402, Test Accuracy: 1.0000\n",
      "Epoch 56/500, Loss: 0.08022835105657578, Test Accuracy: 1.0000\n",
      "Epoch 57/500, Loss: 0.07750434428453445, Test Accuracy: 1.0000\n",
      "Epoch 58/500, Loss: 0.0653933733701706, Test Accuracy: 1.0000\n",
      "Epoch 59/500, Loss: 0.07184839993715286, Test Accuracy: 1.0000\n",
      "Epoch 60/500, Loss: 0.06918047368526459, Test Accuracy: 1.0000\n",
      "Epoch 61/500, Loss: 0.07496095448732376, Test Accuracy: 1.0000\n",
      "Epoch 62/500, Loss: 0.07166760414838791, Test Accuracy: 1.0000\n",
      "Epoch 63/500, Loss: 0.07055460661649704, Test Accuracy: 1.0000\n",
      "Epoch 64/500, Loss: 0.057265784591436386, Test Accuracy: 1.0000\n",
      "Epoch 65/500, Loss: 0.06771552562713623, Test Accuracy: 1.0000\n",
      "Epoch 66/500, Loss: 0.06317541748285294, Test Accuracy: 1.0000\n",
      "Epoch 67/500, Loss: 0.06450402736663818, Test Accuracy: 1.0000\n",
      "Epoch 68/500, Loss: 0.056054215878248215, Test Accuracy: 1.0000\n",
      "Epoch 69/500, Loss: 0.057943206280469894, Test Accuracy: 1.0000\n",
      "Epoch 70/500, Loss: 0.05670050531625748, Test Accuracy: 1.0000\n",
      "Epoch 71/500, Loss: 0.05385790765285492, Test Accuracy: 1.0000\n",
      "Epoch 72/500, Loss: 0.054609619081020355, Test Accuracy: 1.0000\n",
      "Epoch 73/500, Loss: 0.06449881941080093, Test Accuracy: 1.0000\n",
      "Epoch 74/500, Loss: 0.047997698187828064, Test Accuracy: 1.0000\n",
      "Epoch 75/500, Loss: 0.057969555258750916, Test Accuracy: 1.0000\n",
      "Epoch 76/500, Loss: 0.048012979328632355, Test Accuracy: 1.0000\n",
      "Epoch 77/500, Loss: 0.04696634039282799, Test Accuracy: 1.0000\n",
      "Epoch 78/500, Loss: 0.058709342032670975, Test Accuracy: 1.0000\n",
      "Epoch 79/500, Loss: 0.05734966695308685, Test Accuracy: 1.0000\n",
      "Epoch 80/500, Loss: 0.0464148111641407, Test Accuracy: 1.0000\n",
      "Epoch 81/500, Loss: 0.05740831792354584, Test Accuracy: 1.0000\n",
      "Epoch 82/500, Loss: 0.04257692024111748, Test Accuracy: 1.0000\n",
      "Epoch 83/500, Loss: 0.046268340200185776, Test Accuracy: 1.0000\n",
      "Epoch 84/500, Loss: 0.04154052585363388, Test Accuracy: 1.0000\n",
      "Epoch 85/500, Loss: 0.041384078562259674, Test Accuracy: 1.0000\n",
      "Epoch 86/500, Loss: 0.053112681955099106, Test Accuracy: 1.0000\n",
      "Epoch 87/500, Loss: 0.045957066118717194, Test Accuracy: 1.0000\n",
      "Epoch 88/500, Loss: 0.04293597862124443, Test Accuracy: 1.0000\n",
      "Epoch 89/500, Loss: 0.03800123557448387, Test Accuracy: 1.0000\n",
      "Epoch 90/500, Loss: 0.04746821150183678, Test Accuracy: 1.0000\n",
      "Epoch 91/500, Loss: 0.036003101617097855, Test Accuracy: 1.0000\n",
      "Epoch 92/500, Loss: 0.041004765778779984, Test Accuracy: 1.0000\n",
      "Epoch 93/500, Loss: 0.048867110162973404, Test Accuracy: 1.0000\n",
      "Epoch 94/500, Loss: 0.04727347567677498, Test Accuracy: 1.0000\n",
      "Epoch 95/500, Loss: 0.04097847640514374, Test Accuracy: 1.0000\n",
      "Epoch 96/500, Loss: 0.036812473088502884, Test Accuracy: 1.0000\n",
      "Epoch 97/500, Loss: 0.03850827366113663, Test Accuracy: 1.0000\n",
      "Epoch 98/500, Loss: 0.043709296733140945, Test Accuracy: 1.0000\n",
      "Epoch 99/500, Loss: 0.039510201662778854, Test Accuracy: 1.0000\n",
      "Epoch 100/500, Loss: 0.03654223307967186, Test Accuracy: 1.0000\n",
      "Epoch 101/500, Loss: 0.04162725433707237, Test Accuracy: 1.0000\n",
      "Epoch 102/500, Loss: 0.036040257662534714, Test Accuracy: 1.0000\n",
      "Epoch 103/500, Loss: 0.030747149139642715, Test Accuracy: 1.0000\n",
      "Epoch 104/500, Loss: 0.03813985362648964, Test Accuracy: 1.0000\n",
      "Epoch 105/500, Loss: 0.0326533168554306, Test Accuracy: 1.0000\n",
      "Epoch 106/500, Loss: 0.03411410376429558, Test Accuracy: 1.0000\n",
      "Epoch 107/500, Loss: 0.03329101577401161, Test Accuracy: 1.0000\n",
      "Epoch 108/500, Loss: 0.02735457755625248, Test Accuracy: 1.0000\n",
      "Epoch 109/500, Loss: 0.0418485552072525, Test Accuracy: 1.0000\n",
      "Epoch 110/500, Loss: 0.027170538902282715, Test Accuracy: 1.0000\n",
      "Epoch 111/500, Loss: 0.0381881520152092, Test Accuracy: 1.0000\n",
      "Epoch 112/500, Loss: 0.04200838878750801, Test Accuracy: 1.0000\n",
      "Epoch 113/500, Loss: 0.02948141098022461, Test Accuracy: 1.0000\n",
      "Epoch 114/500, Loss: 0.032872095704078674, Test Accuracy: 1.0000\n",
      "Epoch 115/500, Loss: 0.035825710743665695, Test Accuracy: 1.0000\n",
      "Epoch 116/500, Loss: 0.03402112424373627, Test Accuracy: 1.0000\n",
      "Epoch 117/500, Loss: 0.026735618710517883, Test Accuracy: 1.0000\n",
      "Epoch 118/500, Loss: 0.02989034354686737, Test Accuracy: 1.0000\n",
      "Epoch 119/500, Loss: 0.02781752496957779, Test Accuracy: 1.0000\n",
      "Early stopping triggered\n",
      "Best Epoch: 19\n",
      "Confusion Matrix:\n",
      " [[33  0]\n",
      " [ 0 37]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      1.00      1.00        33\n",
      "     Class 1       1.00      1.00      1.00        37\n",
      "\n",
      "    accuracy                           1.00        70\n",
      "   macro avg       1.00      1.00      1.00        70\n",
      "weighted avg       1.00      1.00      1.00        70\n",
      "\n",
      "Best Test Accuracy: 1.0000\n",
      "Fold 7\n",
      "Epoch 1/500, Loss: 0.5840917229652405, Test Accuracy: 0.8857\n",
      "Epoch 2/500, Loss: 0.5169554948806763, Test Accuracy: 0.9429\n",
      "Epoch 3/500, Loss: 0.49539685249328613, Test Accuracy: 0.9571\n",
      "Epoch 4/500, Loss: 0.4335291087627411, Test Accuracy: 1.0000\n",
      "Epoch 5/500, Loss: 0.3964376449584961, Test Accuracy: 1.0000\n",
      "Epoch 6/500, Loss: 0.3700355589389801, Test Accuracy: 1.0000\n",
      "Epoch 7/500, Loss: 0.3348454535007477, Test Accuracy: 1.0000\n",
      "Epoch 8/500, Loss: 0.31049180030822754, Test Accuracy: 1.0000\n",
      "Epoch 9/500, Loss: 0.2801695764064789, Test Accuracy: 1.0000\n",
      "Epoch 10/500, Loss: 0.25716671347618103, Test Accuracy: 1.0000\n",
      "Epoch 11/500, Loss: 0.2394733875989914, Test Accuracy: 1.0000\n",
      "Epoch 12/500, Loss: 0.21728701889514923, Test Accuracy: 1.0000\n",
      "Epoch 13/500, Loss: 0.20486129820346832, Test Accuracy: 1.0000\n",
      "Epoch 14/500, Loss: 0.18491622805595398, Test Accuracy: 1.0000\n",
      "Epoch 15/500, Loss: 0.1729721873998642, Test Accuracy: 1.0000\n",
      "Epoch 16/500, Loss: 0.15060529112815857, Test Accuracy: 1.0000\n",
      "Epoch 17/500, Loss: 0.14994744956493378, Test Accuracy: 1.0000\n",
      "Epoch 18/500, Loss: 0.14684607088565826, Test Accuracy: 1.0000\n",
      "Epoch 19/500, Loss: 0.1256556212902069, Test Accuracy: 1.0000\n",
      "Epoch 20/500, Loss: 0.12843221426010132, Test Accuracy: 1.0000\n",
      "Epoch 21/500, Loss: 0.1126353070139885, Test Accuracy: 1.0000\n",
      "Epoch 22/500, Loss: 0.1120072603225708, Test Accuracy: 1.0000\n",
      "Epoch 23/500, Loss: 0.10156852751970291, Test Accuracy: 1.0000\n",
      "Epoch 24/500, Loss: 0.10133542865514755, Test Accuracy: 1.0000\n",
      "Epoch 25/500, Loss: 0.08647317439317703, Test Accuracy: 1.0000\n",
      "Epoch 26/500, Loss: 0.08487056940793991, Test Accuracy: 1.0000\n",
      "Epoch 27/500, Loss: 0.08471288532018661, Test Accuracy: 1.0000\n",
      "Epoch 28/500, Loss: 0.08052593469619751, Test Accuracy: 1.0000\n",
      "Epoch 29/500, Loss: 0.07453319430351257, Test Accuracy: 1.0000\n",
      "Epoch 30/500, Loss: 0.06702400743961334, Test Accuracy: 1.0000\n",
      "Epoch 31/500, Loss: 0.06474784761667252, Test Accuracy: 1.0000\n",
      "Epoch 32/500, Loss: 0.0656106173992157, Test Accuracy: 1.0000\n",
      "Epoch 33/500, Loss: 0.06334391236305237, Test Accuracy: 1.0000\n",
      "Epoch 34/500, Loss: 0.06052829697728157, Test Accuracy: 1.0000\n",
      "Epoch 35/500, Loss: 0.05438961833715439, Test Accuracy: 1.0000\n",
      "Epoch 36/500, Loss: 0.05479973927140236, Test Accuracy: 1.0000\n",
      "Epoch 37/500, Loss: 0.05967999994754791, Test Accuracy: 1.0000\n",
      "Epoch 38/500, Loss: 0.047238755971193314, Test Accuracy: 1.0000\n",
      "Epoch 39/500, Loss: 0.05161372572183609, Test Accuracy: 1.0000\n",
      "Epoch 40/500, Loss: 0.04682943969964981, Test Accuracy: 1.0000\n",
      "Epoch 41/500, Loss: 0.04295651987195015, Test Accuracy: 1.0000\n",
      "Epoch 42/500, Loss: 0.047014545649290085, Test Accuracy: 1.0000\n",
      "Epoch 43/500, Loss: 0.04417415335774422, Test Accuracy: 1.0000\n",
      "Epoch 44/500, Loss: 0.04098394885659218, Test Accuracy: 1.0000\n",
      "Epoch 45/500, Loss: 0.04101854935288429, Test Accuracy: 1.0000\n",
      "Epoch 46/500, Loss: 0.03898405283689499, Test Accuracy: 1.0000\n",
      "Epoch 47/500, Loss: 0.0379122793674469, Test Accuracy: 1.0000\n",
      "Epoch 48/500, Loss: 0.04483935981988907, Test Accuracy: 1.0000\n",
      "Epoch 49/500, Loss: 0.033709365874528885, Test Accuracy: 1.0000\n",
      "Epoch 50/500, Loss: 0.034152012318372726, Test Accuracy: 1.0000\n",
      "Epoch 51/500, Loss: 0.0382736474275589, Test Accuracy: 1.0000\n",
      "Epoch 52/500, Loss: 0.04086233302950859, Test Accuracy: 1.0000\n",
      "Epoch 53/500, Loss: 0.03324407339096069, Test Accuracy: 1.0000\n",
      "Epoch 54/500, Loss: 0.035370949655771255, Test Accuracy: 1.0000\n",
      "Epoch 55/500, Loss: 0.0348021537065506, Test Accuracy: 1.0000\n",
      "Epoch 56/500, Loss: 0.03130834177136421, Test Accuracy: 1.0000\n",
      "Epoch 57/500, Loss: 0.028235306963324547, Test Accuracy: 1.0000\n",
      "Epoch 58/500, Loss: 0.027103403583168983, Test Accuracy: 1.0000\n",
      "Epoch 59/500, Loss: 0.03612871468067169, Test Accuracy: 1.0000\n",
      "Epoch 60/500, Loss: 0.031415924429893494, Test Accuracy: 1.0000\n",
      "Epoch 61/500, Loss: 0.030536146834492683, Test Accuracy: 1.0000\n",
      "Epoch 62/500, Loss: 0.03990507498383522, Test Accuracy: 1.0000\n",
      "Epoch 63/500, Loss: 0.030583428218960762, Test Accuracy: 1.0000\n",
      "Epoch 64/500, Loss: 0.028631821274757385, Test Accuracy: 1.0000\n",
      "Epoch 65/500, Loss: 0.027149444445967674, Test Accuracy: 1.0000\n",
      "Epoch 66/500, Loss: 0.025959262624382973, Test Accuracy: 1.0000\n",
      "Epoch 67/500, Loss: 0.025484103709459305, Test Accuracy: 1.0000\n",
      "Epoch 68/500, Loss: 0.029375720769166946, Test Accuracy: 1.0000\n",
      "Epoch 69/500, Loss: 0.02934902347624302, Test Accuracy: 1.0000\n",
      "Epoch 70/500, Loss: 0.02421000599861145, Test Accuracy: 1.0000\n",
      "Epoch 71/500, Loss: 0.028772559016942978, Test Accuracy: 1.0000\n",
      "Epoch 72/500, Loss: 0.023424534127116203, Test Accuracy: 1.0000\n",
      "Epoch 73/500, Loss: 0.02079957164824009, Test Accuracy: 1.0000\n",
      "Epoch 74/500, Loss: 0.02265838533639908, Test Accuracy: 1.0000\n",
      "Epoch 75/500, Loss: 0.02523321472108364, Test Accuracy: 1.0000\n",
      "Epoch 76/500, Loss: 0.02719835937023163, Test Accuracy: 1.0000\n",
      "Epoch 77/500, Loss: 0.01953566074371338, Test Accuracy: 1.0000\n",
      "Epoch 78/500, Loss: 0.024143686518073082, Test Accuracy: 1.0000\n",
      "Epoch 79/500, Loss: 0.0210705678910017, Test Accuracy: 1.0000\n",
      "Epoch 80/500, Loss: 0.021971222013235092, Test Accuracy: 1.0000\n",
      "Epoch 81/500, Loss: 0.01902586594223976, Test Accuracy: 1.0000\n",
      "Epoch 82/500, Loss: 0.024878932163119316, Test Accuracy: 1.0000\n",
      "Epoch 83/500, Loss: 0.019667761400341988, Test Accuracy: 1.0000\n",
      "Epoch 84/500, Loss: 0.02511596865952015, Test Accuracy: 1.0000\n",
      "Epoch 85/500, Loss: 0.022220714017748833, Test Accuracy: 1.0000\n",
      "Epoch 86/500, Loss: 0.021785181015729904, Test Accuracy: 1.0000\n",
      "Epoch 87/500, Loss: 0.015972597524523735, Test Accuracy: 1.0000\n",
      "Epoch 88/500, Loss: 0.02296312525868416, Test Accuracy: 1.0000\n",
      "Epoch 89/500, Loss: 0.025579510256648064, Test Accuracy: 1.0000\n",
      "Epoch 90/500, Loss: 0.022294793277978897, Test Accuracy: 1.0000\n",
      "Epoch 91/500, Loss: 0.019766001030802727, Test Accuracy: 1.0000\n",
      "Epoch 92/500, Loss: 0.018617603927850723, Test Accuracy: 1.0000\n",
      "Epoch 93/500, Loss: 0.01433564443141222, Test Accuracy: 1.0000\n",
      "Epoch 94/500, Loss: 0.026002006605267525, Test Accuracy: 1.0000\n",
      "Epoch 95/500, Loss: 0.02337496355175972, Test Accuracy: 1.0000\n",
      "Epoch 96/500, Loss: 0.015317310579121113, Test Accuracy: 1.0000\n",
      "Epoch 97/500, Loss: 0.02219785749912262, Test Accuracy: 1.0000\n",
      "Epoch 98/500, Loss: 0.01996208168566227, Test Accuracy: 1.0000\n",
      "Epoch 99/500, Loss: 0.018559126183390617, Test Accuracy: 1.0000\n",
      "Epoch 100/500, Loss: 0.017687879502773285, Test Accuracy: 1.0000\n",
      "Epoch 101/500, Loss: 0.015520679764449596, Test Accuracy: 1.0000\n",
      "Epoch 102/500, Loss: 0.030629925429821014, Test Accuracy: 1.0000\n",
      "Epoch 103/500, Loss: 0.013463149778544903, Test Accuracy: 1.0000\n",
      "Epoch 104/500, Loss: 0.021202100440859795, Test Accuracy: 1.0000\n",
      "Early stopping triggered\n",
      "Best Epoch: 4\n",
      "Confusion Matrix:\n",
      " [[34  0]\n",
      " [ 0 36]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      1.00      1.00        34\n",
      "     Class 1       1.00      1.00      1.00        36\n",
      "\n",
      "    accuracy                           1.00        70\n",
      "   macro avg       1.00      1.00      1.00        70\n",
      "weighted avg       1.00      1.00      1.00        70\n",
      "\n",
      "Best Test Accuracy: 1.0000\n",
      "Fold 8\n",
      "Epoch 1/500, Loss: 0.802871823310852, Test Accuracy: 0.4714\n",
      "Epoch 2/500, Loss: 0.7214049696922302, Test Accuracy: 0.5714\n",
      "Epoch 3/500, Loss: 0.6538326740264893, Test Accuracy: 0.7286\n",
      "Epoch 4/500, Loss: 0.5988765954971313, Test Accuracy: 0.8143\n",
      "Epoch 5/500, Loss: 0.5367239117622375, Test Accuracy: 0.8714\n",
      "Epoch 6/500, Loss: 0.49353471398353577, Test Accuracy: 0.9143\n",
      "Epoch 7/500, Loss: 0.4511764943599701, Test Accuracy: 0.9429\n",
      "Epoch 8/500, Loss: 0.4088411033153534, Test Accuracy: 0.9429\n",
      "Epoch 9/500, Loss: 0.38108378648757935, Test Accuracy: 0.9571\n",
      "Epoch 10/500, Loss: 0.33705660700798035, Test Accuracy: 0.9571\n",
      "Epoch 11/500, Loss: 0.3364394009113312, Test Accuracy: 0.9429\n",
      "Epoch 12/500, Loss: 0.30549776554107666, Test Accuracy: 0.9429\n",
      "Epoch 13/500, Loss: 0.2757895588874817, Test Accuracy: 0.9429\n",
      "Epoch 14/500, Loss: 0.2534657120704651, Test Accuracy: 0.9429\n",
      "Epoch 15/500, Loss: 0.23533941805362701, Test Accuracy: 0.9714\n",
      "Epoch 16/500, Loss: 0.21483968198299408, Test Accuracy: 0.9714\n",
      "Epoch 17/500, Loss: 0.19406534731388092, Test Accuracy: 0.9714\n",
      "Epoch 18/500, Loss: 0.18678151071071625, Test Accuracy: 0.9714\n",
      "Epoch 19/500, Loss: 0.17153653502464294, Test Accuracy: 0.9857\n",
      "Epoch 20/500, Loss: 0.16258680820465088, Test Accuracy: 0.9857\n",
      "Epoch 21/500, Loss: 0.15127044916152954, Test Accuracy: 0.9857\n",
      "Epoch 22/500, Loss: 0.14106221497058868, Test Accuracy: 0.9857\n",
      "Epoch 23/500, Loss: 0.1344345062971115, Test Accuracy: 0.9857\n",
      "Epoch 24/500, Loss: 0.12414100766181946, Test Accuracy: 0.9857\n",
      "Epoch 25/500, Loss: 0.11536049097776413, Test Accuracy: 0.9857\n",
      "Epoch 26/500, Loss: 0.10668269544839859, Test Accuracy: 0.9857\n",
      "Epoch 27/500, Loss: 0.09493335336446762, Test Accuracy: 0.9857\n",
      "Epoch 28/500, Loss: 0.09848281741142273, Test Accuracy: 0.9857\n",
      "Epoch 29/500, Loss: 0.09420852363109589, Test Accuracy: 0.9857\n",
      "Epoch 30/500, Loss: 0.08993811905384064, Test Accuracy: 0.9857\n",
      "Epoch 31/500, Loss: 0.09058642387390137, Test Accuracy: 0.9857\n",
      "Epoch 32/500, Loss: 0.08095315843820572, Test Accuracy: 0.9857\n",
      "Epoch 33/500, Loss: 0.08065685629844666, Test Accuracy: 0.9857\n",
      "Epoch 34/500, Loss: 0.06936565041542053, Test Accuracy: 0.9857\n",
      "Epoch 35/500, Loss: 0.07172415405511856, Test Accuracy: 0.9857\n",
      "Epoch 36/500, Loss: 0.07001999020576477, Test Accuracy: 0.9857\n",
      "Epoch 37/500, Loss: 0.06512534618377686, Test Accuracy: 0.9857\n",
      "Epoch 38/500, Loss: 0.06881133466959, Test Accuracy: 0.9857\n",
      "Epoch 39/500, Loss: 0.06059017404913902, Test Accuracy: 0.9857\n",
      "Epoch 40/500, Loss: 0.06476889550685883, Test Accuracy: 0.9857\n",
      "Epoch 41/500, Loss: 0.054332878440618515, Test Accuracy: 0.9857\n",
      "Epoch 42/500, Loss: 0.050433021038770676, Test Accuracy: 0.9857\n",
      "Epoch 43/500, Loss: 0.05658181011676788, Test Accuracy: 0.9857\n",
      "Epoch 44/500, Loss: 0.0497228242456913, Test Accuracy: 0.9857\n",
      "Epoch 45/500, Loss: 0.0525236651301384, Test Accuracy: 0.9857\n",
      "Epoch 46/500, Loss: 0.057695936411619186, Test Accuracy: 0.9857\n",
      "Epoch 47/500, Loss: 0.044217102229595184, Test Accuracy: 0.9857\n",
      "Epoch 48/500, Loss: 0.04219222068786621, Test Accuracy: 0.9857\n",
      "Epoch 49/500, Loss: 0.04041079059243202, Test Accuracy: 0.9857\n",
      "Epoch 50/500, Loss: 0.045906275510787964, Test Accuracy: 0.9857\n",
      "Epoch 51/500, Loss: 0.039355430752038956, Test Accuracy: 0.9857\n",
      "Epoch 52/500, Loss: 0.04034135490655899, Test Accuracy: 0.9857\n",
      "Epoch 53/500, Loss: 0.037818294018507004, Test Accuracy: 0.9857\n",
      "Epoch 54/500, Loss: 0.038741111755371094, Test Accuracy: 0.9857\n",
      "Epoch 55/500, Loss: 0.04005340114235878, Test Accuracy: 0.9857\n",
      "Epoch 56/500, Loss: 0.03544442355632782, Test Accuracy: 0.9857\n",
      "Epoch 57/500, Loss: 0.03115210309624672, Test Accuracy: 0.9857\n",
      "Epoch 58/500, Loss: 0.03145367652177811, Test Accuracy: 0.9857\n",
      "Epoch 59/500, Loss: 0.04008791595697403, Test Accuracy: 0.9857\n",
      "Epoch 60/500, Loss: 0.03447211533784866, Test Accuracy: 0.9857\n",
      "Epoch 61/500, Loss: 0.03071773238480091, Test Accuracy: 0.9857\n",
      "Epoch 62/500, Loss: 0.03542863205075264, Test Accuracy: 0.9857\n",
      "Epoch 63/500, Loss: 0.03209839016199112, Test Accuracy: 0.9857\n",
      "Epoch 64/500, Loss: 0.029318735003471375, Test Accuracy: 0.9857\n",
      "Epoch 65/500, Loss: 0.026901056990027428, Test Accuracy: 0.9857\n",
      "Epoch 66/500, Loss: 0.035397835075855255, Test Accuracy: 0.9857\n",
      "Epoch 67/500, Loss: 0.029332520440220833, Test Accuracy: 0.9857\n",
      "Epoch 68/500, Loss: 0.029963266104459763, Test Accuracy: 0.9857\n",
      "Epoch 69/500, Loss: 0.02729071117937565, Test Accuracy: 0.9714\n",
      "Epoch 70/500, Loss: 0.03212926536798477, Test Accuracy: 0.9714\n",
      "Epoch 71/500, Loss: 0.023584168404340744, Test Accuracy: 0.9714\n",
      "Epoch 72/500, Loss: 0.02590220980346203, Test Accuracy: 0.9714\n",
      "Epoch 73/500, Loss: 0.031193850561976433, Test Accuracy: 0.9714\n",
      "Epoch 74/500, Loss: 0.02742197923362255, Test Accuracy: 0.9714\n",
      "Epoch 75/500, Loss: 0.02334175631403923, Test Accuracy: 0.9714\n",
      "Epoch 76/500, Loss: 0.02382422238588333, Test Accuracy: 0.9714\n",
      "Epoch 77/500, Loss: 0.024926241487264633, Test Accuracy: 0.9714\n",
      "Epoch 78/500, Loss: 0.026007480919361115, Test Accuracy: 0.9714\n",
      "Epoch 79/500, Loss: 0.021562620997428894, Test Accuracy: 0.9714\n",
      "Epoch 80/500, Loss: 0.02396867424249649, Test Accuracy: 0.9714\n",
      "Epoch 81/500, Loss: 0.02110147289931774, Test Accuracy: 0.9714\n",
      "Epoch 82/500, Loss: 0.023752128705382347, Test Accuracy: 0.9714\n",
      "Epoch 83/500, Loss: 0.023965386673808098, Test Accuracy: 0.9714\n",
      "Epoch 84/500, Loss: 0.024212421849370003, Test Accuracy: 0.9714\n",
      "Epoch 85/500, Loss: 0.02019863948225975, Test Accuracy: 0.9714\n",
      "Epoch 86/500, Loss: 0.02508210390806198, Test Accuracy: 0.9714\n",
      "Epoch 87/500, Loss: 0.023053163662552834, Test Accuracy: 0.9714\n",
      "Epoch 88/500, Loss: 0.021635714918375015, Test Accuracy: 0.9714\n",
      "Epoch 89/500, Loss: 0.02065352536737919, Test Accuracy: 0.9714\n",
      "Epoch 90/500, Loss: 0.019145969301462173, Test Accuracy: 0.9714\n",
      "Epoch 91/500, Loss: 0.018743392080068588, Test Accuracy: 0.9714\n",
      "Epoch 92/500, Loss: 0.019321367144584656, Test Accuracy: 0.9714\n",
      "Epoch 93/500, Loss: 0.019438710063695908, Test Accuracy: 0.9714\n",
      "Epoch 94/500, Loss: 0.019167669117450714, Test Accuracy: 0.9714\n",
      "Epoch 95/500, Loss: 0.017419135197997093, Test Accuracy: 0.9714\n",
      "Epoch 96/500, Loss: 0.01600576564669609, Test Accuracy: 0.9714\n",
      "Epoch 97/500, Loss: 0.020267097279429436, Test Accuracy: 0.9714\n",
      "Epoch 98/500, Loss: 0.016088111326098442, Test Accuracy: 0.9714\n",
      "Epoch 99/500, Loss: 0.017976360395550728, Test Accuracy: 0.9714\n",
      "Epoch 100/500, Loss: 0.017280342057347298, Test Accuracy: 0.9714\n",
      "Epoch 101/500, Loss: 0.016220996156334877, Test Accuracy: 0.9714\n",
      "Epoch 102/500, Loss: 0.016779407858848572, Test Accuracy: 0.9714\n",
      "Epoch 103/500, Loss: 0.020725354552268982, Test Accuracy: 0.9714\n",
      "Epoch 104/500, Loss: 0.02538353204727173, Test Accuracy: 0.9714\n",
      "Epoch 105/500, Loss: 0.014156260527670383, Test Accuracy: 0.9714\n",
      "Epoch 106/500, Loss: 0.01587398163974285, Test Accuracy: 0.9714\n",
      "Epoch 107/500, Loss: 0.013644447550177574, Test Accuracy: 0.9714\n",
      "Epoch 108/500, Loss: 0.015609104186296463, Test Accuracy: 0.9714\n",
      "Epoch 109/500, Loss: 0.017570478841662407, Test Accuracy: 0.9714\n",
      "Epoch 110/500, Loss: 0.014672870747745037, Test Accuracy: 0.9714\n",
      "Epoch 111/500, Loss: 0.017673611640930176, Test Accuracy: 0.9714\n",
      "Epoch 112/500, Loss: 0.022688543424010277, Test Accuracy: 0.9714\n",
      "Epoch 113/500, Loss: 0.019390275701880455, Test Accuracy: 0.9714\n",
      "Epoch 114/500, Loss: 0.014990018680691719, Test Accuracy: 0.9714\n",
      "Epoch 115/500, Loss: 0.017337290570139885, Test Accuracy: 0.9714\n",
      "Epoch 116/500, Loss: 0.013919319026172161, Test Accuracy: 0.9714\n",
      "Epoch 117/500, Loss: 0.011481604538857937, Test Accuracy: 0.9714\n",
      "Epoch 118/500, Loss: 0.01288190670311451, Test Accuracy: 0.9714\n",
      "Epoch 119/500, Loss: 0.02248481847345829, Test Accuracy: 0.9714\n",
      "Early stopping triggered\n",
      "Best Epoch: 19\n",
      "Confusion Matrix:\n",
      " [[30  1]\n",
      " [ 1 38]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.97      0.97      0.97        31\n",
      "     Class 1       0.97      0.97      0.97        39\n",
      "\n",
      "    accuracy                           0.97        70\n",
      "   macro avg       0.97      0.97      0.97        70\n",
      "weighted avg       0.97      0.97      0.97        70\n",
      "\n",
      "Best Test Accuracy: 0.9714\n",
      "Fold 9\n",
      "Epoch 1/500, Loss: 1.0721027851104736, Test Accuracy: 0.0857\n",
      "Epoch 2/500, Loss: 0.9660698771476746, Test Accuracy: 0.2143\n",
      "Epoch 3/500, Loss: 0.9001898169517517, Test Accuracy: 0.2857\n",
      "Epoch 4/500, Loss: 0.8153105974197388, Test Accuracy: 0.4857\n",
      "Epoch 5/500, Loss: 0.7149772644042969, Test Accuracy: 0.5857\n",
      "Epoch 6/500, Loss: 0.6497566103935242, Test Accuracy: 0.6429\n",
      "Epoch 7/500, Loss: 0.6053350567817688, Test Accuracy: 0.7429\n",
      "Epoch 8/500, Loss: 0.5467906594276428, Test Accuracy: 0.8286\n",
      "Epoch 9/500, Loss: 0.4729790985584259, Test Accuracy: 0.8714\n",
      "Epoch 10/500, Loss: 0.44161689281463623, Test Accuracy: 0.9143\n",
      "Epoch 11/500, Loss: 0.40020161867141724, Test Accuracy: 0.9571\n",
      "Epoch 12/500, Loss: 0.3605180084705353, Test Accuracy: 0.9857\n",
      "Epoch 13/500, Loss: 0.3516677916049957, Test Accuracy: 1.0000\n",
      "Epoch 14/500, Loss: 0.3125845193862915, Test Accuracy: 1.0000\n",
      "Epoch 15/500, Loss: 0.2795029878616333, Test Accuracy: 1.0000\n",
      "Epoch 16/500, Loss: 0.2638542950153351, Test Accuracy: 1.0000\n",
      "Epoch 17/500, Loss: 0.23950190842151642, Test Accuracy: 1.0000\n",
      "Epoch 18/500, Loss: 0.22580654919147491, Test Accuracy: 1.0000\n",
      "Epoch 19/500, Loss: 0.21000120043754578, Test Accuracy: 1.0000\n",
      "Epoch 20/500, Loss: 0.19248279929161072, Test Accuracy: 1.0000\n",
      "Epoch 21/500, Loss: 0.18244831264019012, Test Accuracy: 1.0000\n",
      "Epoch 22/500, Loss: 0.16399899125099182, Test Accuracy: 1.0000\n",
      "Epoch 23/500, Loss: 0.16211234033107758, Test Accuracy: 1.0000\n",
      "Epoch 24/500, Loss: 0.15402093529701233, Test Accuracy: 1.0000\n",
      "Epoch 25/500, Loss: 0.142930805683136, Test Accuracy: 1.0000\n",
      "Epoch 26/500, Loss: 0.12594452500343323, Test Accuracy: 1.0000\n",
      "Epoch 27/500, Loss: 0.1297118365764618, Test Accuracy: 1.0000\n",
      "Epoch 28/500, Loss: 0.11957181990146637, Test Accuracy: 1.0000\n",
      "Epoch 29/500, Loss: 0.12471651285886765, Test Accuracy: 1.0000\n",
      "Epoch 30/500, Loss: 0.10272746533155441, Test Accuracy: 1.0000\n",
      "Epoch 31/500, Loss: 0.10578805208206177, Test Accuracy: 1.0000\n",
      "Epoch 32/500, Loss: 0.09937745332717896, Test Accuracy: 1.0000\n",
      "Epoch 33/500, Loss: 0.09482472389936447, Test Accuracy: 1.0000\n",
      "Epoch 34/500, Loss: 0.08377859741449356, Test Accuracy: 1.0000\n",
      "Epoch 35/500, Loss: 0.08723551034927368, Test Accuracy: 1.0000\n",
      "Epoch 36/500, Loss: 0.07942252606153488, Test Accuracy: 1.0000\n",
      "Epoch 37/500, Loss: 0.07755569368600845, Test Accuracy: 1.0000\n",
      "Epoch 38/500, Loss: 0.07843039184808731, Test Accuracy: 1.0000\n",
      "Epoch 39/500, Loss: 0.07782097160816193, Test Accuracy: 1.0000\n",
      "Epoch 40/500, Loss: 0.06908144056797028, Test Accuracy: 1.0000\n",
      "Epoch 41/500, Loss: 0.0706162303686142, Test Accuracy: 1.0000\n",
      "Epoch 42/500, Loss: 0.059482358396053314, Test Accuracy: 1.0000\n",
      "Epoch 43/500, Loss: 0.06366055458784103, Test Accuracy: 1.0000\n",
      "Epoch 44/500, Loss: 0.05967738851904869, Test Accuracy: 1.0000\n",
      "Epoch 45/500, Loss: 0.056557826697826385, Test Accuracy: 1.0000\n",
      "Epoch 46/500, Loss: 0.06454284489154816, Test Accuracy: 1.0000\n",
      "Epoch 47/500, Loss: 0.054190102964639664, Test Accuracy: 1.0000\n",
      "Epoch 48/500, Loss: 0.0651913583278656, Test Accuracy: 1.0000\n",
      "Epoch 49/500, Loss: 0.05379640683531761, Test Accuracy: 1.0000\n",
      "Epoch 50/500, Loss: 0.053644511848688126, Test Accuracy: 1.0000\n",
      "Epoch 51/500, Loss: 0.050517916679382324, Test Accuracy: 1.0000\n",
      "Epoch 52/500, Loss: 0.05021318420767784, Test Accuracy: 1.0000\n",
      "Epoch 53/500, Loss: 0.042339470237493515, Test Accuracy: 1.0000\n",
      "Epoch 54/500, Loss: 0.05657072365283966, Test Accuracy: 1.0000\n",
      "Epoch 55/500, Loss: 0.04787469655275345, Test Accuracy: 1.0000\n",
      "Epoch 56/500, Loss: 0.04483971744775772, Test Accuracy: 1.0000\n",
      "Epoch 57/500, Loss: 0.04073965176939964, Test Accuracy: 1.0000\n",
      "Epoch 58/500, Loss: 0.03934820368885994, Test Accuracy: 1.0000\n",
      "Epoch 59/500, Loss: 0.0417373962700367, Test Accuracy: 1.0000\n",
      "Epoch 60/500, Loss: 0.042385034263134, Test Accuracy: 1.0000\n",
      "Epoch 61/500, Loss: 0.042848147451877594, Test Accuracy: 1.0000\n",
      "Epoch 62/500, Loss: 0.03671707585453987, Test Accuracy: 1.0000\n",
      "Epoch 63/500, Loss: 0.04202678054571152, Test Accuracy: 1.0000\n",
      "Epoch 64/500, Loss: 0.03871528059244156, Test Accuracy: 1.0000\n",
      "Epoch 65/500, Loss: 0.049225371330976486, Test Accuracy: 1.0000\n",
      "Epoch 66/500, Loss: 0.03436731919646263, Test Accuracy: 1.0000\n",
      "Epoch 67/500, Loss: 0.03169136121869087, Test Accuracy: 1.0000\n",
      "Epoch 68/500, Loss: 0.032695379108190536, Test Accuracy: 1.0000\n",
      "Epoch 69/500, Loss: 0.03589939326047897, Test Accuracy: 1.0000\n",
      "Epoch 70/500, Loss: 0.03336423635482788, Test Accuracy: 1.0000\n",
      "Epoch 71/500, Loss: 0.032802265137434006, Test Accuracy: 1.0000\n",
      "Epoch 72/500, Loss: 0.03220304846763611, Test Accuracy: 1.0000\n",
      "Epoch 73/500, Loss: 0.030967002734541893, Test Accuracy: 1.0000\n",
      "Epoch 74/500, Loss: 0.02608533389866352, Test Accuracy: 1.0000\n",
      "Epoch 75/500, Loss: 0.026747452095150948, Test Accuracy: 1.0000\n",
      "Epoch 76/500, Loss: 0.028588714078068733, Test Accuracy: 1.0000\n",
      "Epoch 77/500, Loss: 0.023205770179629326, Test Accuracy: 1.0000\n",
      "Epoch 78/500, Loss: 0.032300665974617004, Test Accuracy: 1.0000\n",
      "Epoch 79/500, Loss: 0.02588237263262272, Test Accuracy: 1.0000\n",
      "Epoch 80/500, Loss: 0.031112635508179665, Test Accuracy: 1.0000\n",
      "Epoch 81/500, Loss: 0.030423631891608238, Test Accuracy: 1.0000\n",
      "Epoch 82/500, Loss: 0.030000794678926468, Test Accuracy: 1.0000\n",
      "Epoch 83/500, Loss: 0.028692565858364105, Test Accuracy: 1.0000\n",
      "Epoch 84/500, Loss: 0.03315427526831627, Test Accuracy: 1.0000\n",
      "Epoch 85/500, Loss: 0.025833353400230408, Test Accuracy: 1.0000\n",
      "Epoch 86/500, Loss: 0.02559165470302105, Test Accuracy: 1.0000\n",
      "Epoch 87/500, Loss: 0.028113864362239838, Test Accuracy: 1.0000\n",
      "Epoch 88/500, Loss: 0.021684393286705017, Test Accuracy: 1.0000\n",
      "Epoch 89/500, Loss: 0.027585122734308243, Test Accuracy: 1.0000\n",
      "Epoch 90/500, Loss: 0.030225230380892754, Test Accuracy: 1.0000\n",
      "Epoch 91/500, Loss: 0.02044130302965641, Test Accuracy: 1.0000\n",
      "Epoch 92/500, Loss: 0.02891266532242298, Test Accuracy: 1.0000\n",
      "Epoch 93/500, Loss: 0.02296121045947075, Test Accuracy: 1.0000\n",
      "Epoch 94/500, Loss: 0.022335520014166832, Test Accuracy: 1.0000\n",
      "Epoch 95/500, Loss: 0.022773992270231247, Test Accuracy: 1.0000\n",
      "Epoch 96/500, Loss: 0.020072976127266884, Test Accuracy: 1.0000\n",
      "Epoch 97/500, Loss: 0.017998026683926582, Test Accuracy: 1.0000\n",
      "Epoch 98/500, Loss: 0.026215406134724617, Test Accuracy: 1.0000\n",
      "Epoch 99/500, Loss: 0.023852510377764702, Test Accuracy: 1.0000\n",
      "Epoch 100/500, Loss: 0.02267388068139553, Test Accuracy: 1.0000\n",
      "Epoch 101/500, Loss: 0.022136561572551727, Test Accuracy: 1.0000\n",
      "Epoch 102/500, Loss: 0.016896339133381844, Test Accuracy: 1.0000\n",
      "Epoch 103/500, Loss: 0.021297713741660118, Test Accuracy: 1.0000\n",
      "Epoch 104/500, Loss: 0.015548106282949448, Test Accuracy: 1.0000\n",
      "Epoch 105/500, Loss: 0.01673297956585884, Test Accuracy: 1.0000\n",
      "Epoch 106/500, Loss: 0.01765669882297516, Test Accuracy: 1.0000\n",
      "Epoch 107/500, Loss: 0.02034742943942547, Test Accuracy: 1.0000\n",
      "Epoch 108/500, Loss: 0.023883763700723648, Test Accuracy: 1.0000\n",
      "Epoch 109/500, Loss: 0.02137802541255951, Test Accuracy: 1.0000\n",
      "Epoch 110/500, Loss: 0.01574781909584999, Test Accuracy: 1.0000\n",
      "Epoch 111/500, Loss: 0.02115773782134056, Test Accuracy: 1.0000\n",
      "Epoch 112/500, Loss: 0.019538946449756622, Test Accuracy: 1.0000\n",
      "Epoch 113/500, Loss: 0.018587587401270866, Test Accuracy: 1.0000\n",
      "Early stopping triggered\n",
      "Best Epoch: 13\n",
      "Confusion Matrix:\n",
      " [[37  0]\n",
      " [ 0 33]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      1.00      1.00        37\n",
      "     Class 1       1.00      1.00      1.00        33\n",
      "\n",
      "    accuracy                           1.00        70\n",
      "   macro avg       1.00      1.00      1.00        70\n",
      "weighted avg       1.00      1.00      1.00        70\n",
      "\n",
      "Best Test Accuracy: 1.0000\n",
      "Fold 10\n",
      "Epoch 1/500, Loss: 1.863097071647644, Test Accuracy: 0.0143\n",
      "Epoch 2/500, Loss: 1.718374252319336, Test Accuracy: 0.0143\n",
      "Epoch 3/500, Loss: 1.6076303720474243, Test Accuracy: 0.0286\n",
      "Epoch 4/500, Loss: 1.4703278541564941, Test Accuracy: 0.0286\n",
      "Epoch 5/500, Loss: 1.3570266962051392, Test Accuracy: 0.0429\n",
      "Epoch 6/500, Loss: 1.2611660957336426, Test Accuracy: 0.0571\n",
      "Epoch 7/500, Loss: 1.1362481117248535, Test Accuracy: 0.1286\n",
      "Epoch 8/500, Loss: 1.0365984439849854, Test Accuracy: 0.2286\n",
      "Epoch 9/500, Loss: 0.9022511839866638, Test Accuracy: 0.3857\n",
      "Epoch 10/500, Loss: 0.86383056640625, Test Accuracy: 0.4714\n",
      "Epoch 11/500, Loss: 0.7713811993598938, Test Accuracy: 0.6571\n",
      "Epoch 12/500, Loss: 0.7182333469390869, Test Accuracy: 0.7714\n",
      "Epoch 13/500, Loss: 0.6604076027870178, Test Accuracy: 0.8571\n",
      "Epoch 14/500, Loss: 0.6089599132537842, Test Accuracy: 0.9000\n",
      "Epoch 15/500, Loss: 0.5687354207038879, Test Accuracy: 0.9286\n",
      "Epoch 16/500, Loss: 0.5288240909576416, Test Accuracy: 0.9429\n",
      "Epoch 17/500, Loss: 0.4921817481517792, Test Accuracy: 0.9429\n",
      "Epoch 18/500, Loss: 0.4591725468635559, Test Accuracy: 0.9571\n",
      "Epoch 19/500, Loss: 0.4326736032962799, Test Accuracy: 0.9571\n",
      "Epoch 20/500, Loss: 0.40530505776405334, Test Accuracy: 0.9714\n",
      "Epoch 21/500, Loss: 0.3658253848552704, Test Accuracy: 0.9714\n",
      "Epoch 22/500, Loss: 0.34324806928634644, Test Accuracy: 0.9714\n",
      "Epoch 23/500, Loss: 0.31748729944229126, Test Accuracy: 0.9857\n",
      "Epoch 24/500, Loss: 0.2942623794078827, Test Accuracy: 0.9857\n",
      "Epoch 25/500, Loss: 0.26445379853248596, Test Accuracy: 0.9857\n",
      "Epoch 26/500, Loss: 0.26106035709381104, Test Accuracy: 0.9857\n",
      "Epoch 27/500, Loss: 0.2521711587905884, Test Accuracy: 0.9857\n",
      "Epoch 28/500, Loss: 0.22730344533920288, Test Accuracy: 0.9857\n",
      "Epoch 29/500, Loss: 0.21555672585964203, Test Accuracy: 0.9857\n",
      "Epoch 30/500, Loss: 0.213038831949234, Test Accuracy: 0.9857\n",
      "Epoch 31/500, Loss: 0.19920766353607178, Test Accuracy: 0.9857\n",
      "Epoch 32/500, Loss: 0.19139541685581207, Test Accuracy: 0.9857\n",
      "Epoch 33/500, Loss: 0.17766866087913513, Test Accuracy: 0.9857\n",
      "Epoch 34/500, Loss: 0.16091115772724152, Test Accuracy: 0.9857\n",
      "Epoch 35/500, Loss: 0.14995746314525604, Test Accuracy: 0.9857\n",
      "Epoch 36/500, Loss: 0.1426847279071808, Test Accuracy: 0.9857\n",
      "Epoch 37/500, Loss: 0.14058516919612885, Test Accuracy: 0.9857\n",
      "Epoch 38/500, Loss: 0.12939871847629547, Test Accuracy: 1.0000\n",
      "Epoch 39/500, Loss: 0.13340504467487335, Test Accuracy: 1.0000\n",
      "Epoch 40/500, Loss: 0.11464005708694458, Test Accuracy: 1.0000\n",
      "Epoch 41/500, Loss: 0.10354436188936234, Test Accuracy: 1.0000\n",
      "Epoch 42/500, Loss: 0.11517372727394104, Test Accuracy: 0.9857\n",
      "Epoch 43/500, Loss: 0.1011202409863472, Test Accuracy: 0.9857\n",
      "Epoch 44/500, Loss: 0.09506639838218689, Test Accuracy: 0.9857\n",
      "Epoch 45/500, Loss: 0.08998674899339676, Test Accuracy: 0.9857\n",
      "Epoch 46/500, Loss: 0.08909406512975693, Test Accuracy: 0.9857\n",
      "Epoch 47/500, Loss: 0.09454233199357986, Test Accuracy: 0.9857\n",
      "Epoch 48/500, Loss: 0.08264142274856567, Test Accuracy: 0.9857\n",
      "Epoch 49/500, Loss: 0.07782566547393799, Test Accuracy: 0.9857\n",
      "Epoch 50/500, Loss: 0.07786569744348526, Test Accuracy: 0.9857\n",
      "Epoch 51/500, Loss: 0.06953755021095276, Test Accuracy: 1.0000\n",
      "Epoch 52/500, Loss: 0.06471686065196991, Test Accuracy: 1.0000\n",
      "Epoch 53/500, Loss: 0.06881240010261536, Test Accuracy: 1.0000\n",
      "Epoch 54/500, Loss: 0.06159159913659096, Test Accuracy: 1.0000\n",
      "Epoch 55/500, Loss: 0.06902898848056793, Test Accuracy: 1.0000\n",
      "Epoch 56/500, Loss: 0.06323069334030151, Test Accuracy: 1.0000\n",
      "Epoch 57/500, Loss: 0.06299872696399689, Test Accuracy: 1.0000\n",
      "Epoch 58/500, Loss: 0.05477118864655495, Test Accuracy: 1.0000\n",
      "Epoch 59/500, Loss: 0.06304121762514114, Test Accuracy: 1.0000\n",
      "Epoch 60/500, Loss: 0.05559975653886795, Test Accuracy: 1.0000\n",
      "Epoch 61/500, Loss: 0.046190157532691956, Test Accuracy: 1.0000\n",
      "Epoch 62/500, Loss: 0.060392025858163834, Test Accuracy: 1.0000\n",
      "Epoch 63/500, Loss: 0.04421374574303627, Test Accuracy: 1.0000\n",
      "Epoch 64/500, Loss: 0.0464998260140419, Test Accuracy: 1.0000\n",
      "Epoch 65/500, Loss: 0.04681483656167984, Test Accuracy: 1.0000\n",
      "Epoch 66/500, Loss: 0.04801246523857117, Test Accuracy: 1.0000\n",
      "Epoch 67/500, Loss: 0.047315143048763275, Test Accuracy: 1.0000\n",
      "Epoch 68/500, Loss: 0.0424487441778183, Test Accuracy: 1.0000\n",
      "Epoch 69/500, Loss: 0.046336010098457336, Test Accuracy: 1.0000\n",
      "Epoch 70/500, Loss: 0.03943416848778725, Test Accuracy: 1.0000\n",
      "Epoch 71/500, Loss: 0.03859305381774902, Test Accuracy: 1.0000\n",
      "Epoch 72/500, Loss: 0.04076123982667923, Test Accuracy: 1.0000\n",
      "Epoch 73/500, Loss: 0.03977452218532562, Test Accuracy: 1.0000\n",
      "Epoch 74/500, Loss: 0.04145191237330437, Test Accuracy: 1.0000\n",
      "Epoch 75/500, Loss: 0.03642194718122482, Test Accuracy: 1.0000\n",
      "Epoch 76/500, Loss: 0.03909190371632576, Test Accuracy: 1.0000\n",
      "Epoch 77/500, Loss: 0.03560442477464676, Test Accuracy: 1.0000\n",
      "Epoch 78/500, Loss: 0.032014790922403336, Test Accuracy: 1.0000\n",
      "Epoch 79/500, Loss: 0.03613600879907608, Test Accuracy: 1.0000\n",
      "Epoch 80/500, Loss: 0.03376355022192001, Test Accuracy: 1.0000\n",
      "Epoch 81/500, Loss: 0.035399097949266434, Test Accuracy: 1.0000\n",
      "Epoch 82/500, Loss: 0.03393044322729111, Test Accuracy: 1.0000\n",
      "Epoch 83/500, Loss: 0.03131226822733879, Test Accuracy: 1.0000\n",
      "Epoch 84/500, Loss: 0.03618967905640602, Test Accuracy: 1.0000\n",
      "Epoch 85/500, Loss: 0.027875453233718872, Test Accuracy: 1.0000\n",
      "Epoch 86/500, Loss: 0.029341448098421097, Test Accuracy: 1.0000\n",
      "Epoch 87/500, Loss: 0.03153079003095627, Test Accuracy: 1.0000\n",
      "Epoch 88/500, Loss: 0.028872419148683548, Test Accuracy: 1.0000\n",
      "Epoch 89/500, Loss: 0.026444727554917336, Test Accuracy: 1.0000\n",
      "Epoch 90/500, Loss: 0.03152313083410263, Test Accuracy: 1.0000\n",
      "Epoch 91/500, Loss: 0.03040432371199131, Test Accuracy: 1.0000\n",
      "Epoch 92/500, Loss: 0.028234632685780525, Test Accuracy: 1.0000\n",
      "Epoch 93/500, Loss: 0.027617204934358597, Test Accuracy: 1.0000\n",
      "Epoch 94/500, Loss: 0.034116074442863464, Test Accuracy: 1.0000\n",
      "Epoch 95/500, Loss: 0.026513060554862022, Test Accuracy: 1.0000\n",
      "Epoch 96/500, Loss: 0.02656790427863598, Test Accuracy: 1.0000\n",
      "Epoch 97/500, Loss: 0.02593475580215454, Test Accuracy: 1.0000\n",
      "Epoch 98/500, Loss: 0.025413643568754196, Test Accuracy: 1.0000\n",
      "Epoch 99/500, Loss: 0.026160161942243576, Test Accuracy: 1.0000\n",
      "Epoch 100/500, Loss: 0.026530684903264046, Test Accuracy: 1.0000\n",
      "Epoch 101/500, Loss: 0.024305593222379684, Test Accuracy: 0.9857\n",
      "Epoch 102/500, Loss: 0.021928027272224426, Test Accuracy: 0.9857\n",
      "Epoch 103/500, Loss: 0.021520668640732765, Test Accuracy: 0.9857\n",
      "Epoch 104/500, Loss: 0.02484363131225109, Test Accuracy: 0.9857\n",
      "Epoch 105/500, Loss: 0.024934235960245132, Test Accuracy: 0.9857\n",
      "Epoch 106/500, Loss: 0.018498174846172333, Test Accuracy: 0.9857\n",
      "Epoch 107/500, Loss: 0.019867783412337303, Test Accuracy: 0.9857\n",
      "Epoch 108/500, Loss: 0.024272752925753593, Test Accuracy: 0.9857\n",
      "Epoch 109/500, Loss: 0.020753413438796997, Test Accuracy: 0.9857\n",
      "Epoch 110/500, Loss: 0.019185572862625122, Test Accuracy: 0.9857\n",
      "Epoch 111/500, Loss: 0.021837430074810982, Test Accuracy: 0.9857\n",
      "Epoch 112/500, Loss: 0.020032240077853203, Test Accuracy: 0.9857\n",
      "Epoch 113/500, Loss: 0.0204286091029644, Test Accuracy: 0.9857\n",
      "Epoch 114/500, Loss: 0.01831403374671936, Test Accuracy: 0.9857\n",
      "Epoch 115/500, Loss: 0.022707557305693626, Test Accuracy: 0.9857\n",
      "Epoch 116/500, Loss: 0.01762518845498562, Test Accuracy: 0.9857\n",
      "Epoch 117/500, Loss: 0.02266502007842064, Test Accuracy: 0.9857\n",
      "Epoch 118/500, Loss: 0.021579867228865623, Test Accuracy: 0.9857\n",
      "Epoch 119/500, Loss: 0.01761702634394169, Test Accuracy: 0.9857\n",
      "Epoch 120/500, Loss: 0.024110421538352966, Test Accuracy: 0.9857\n",
      "Epoch 121/500, Loss: 0.016554243862628937, Test Accuracy: 0.9857\n",
      "Epoch 122/500, Loss: 0.018733954057097435, Test Accuracy: 0.9857\n",
      "Epoch 123/500, Loss: 0.020195115357637405, Test Accuracy: 0.9857\n",
      "Epoch 124/500, Loss: 0.01978774182498455, Test Accuracy: 0.9857\n",
      "Epoch 125/500, Loss: 0.024326946586370468, Test Accuracy: 0.9857\n",
      "Epoch 126/500, Loss: 0.018726840615272522, Test Accuracy: 0.9857\n",
      "Epoch 127/500, Loss: 0.016524069011211395, Test Accuracy: 1.0000\n",
      "Epoch 128/500, Loss: 0.01660190522670746, Test Accuracy: 1.0000\n",
      "Epoch 129/500, Loss: 0.020928310230374336, Test Accuracy: 1.0000\n",
      "Epoch 130/500, Loss: 0.018676262348890305, Test Accuracy: 1.0000\n",
      "Epoch 131/500, Loss: 0.018255168572068214, Test Accuracy: 0.9857\n",
      "Epoch 132/500, Loss: 0.01705239899456501, Test Accuracy: 0.9857\n",
      "Epoch 133/500, Loss: 0.017503827810287476, Test Accuracy: 0.9857\n",
      "Epoch 134/500, Loss: 0.019190208986401558, Test Accuracy: 0.9857\n",
      "Epoch 135/500, Loss: 0.015261808410286903, Test Accuracy: 0.9857\n",
      "Epoch 136/500, Loss: 0.013426934368908405, Test Accuracy: 0.9857\n",
      "Epoch 137/500, Loss: 0.019503364339470863, Test Accuracy: 0.9857\n",
      "Epoch 138/500, Loss: 0.01406722329556942, Test Accuracy: 0.9857\n",
      "Early stopping triggered\n",
      "Best Epoch: 38\n",
      "Confusion Matrix:\n",
      " [[28  0]\n",
      " [ 1 41]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.97      1.00      0.98        28\n",
      "     Class 1       1.00      0.98      0.99        42\n",
      "\n",
      "    accuracy                           0.99        70\n",
      "   macro avg       0.98      0.99      0.99        70\n",
      "weighted avg       0.99      0.99      0.99        70\n",
      "\n",
      "Best Test Accuracy: 0.9857\n",
      "\n",
      "Cross-Validation Results:\n",
      "Average Accuracy: 0.94215291750503\n",
      "Average Confusion Matrix:\n",
      " [[32.1  2.5]\n",
      " [ 1.6 34.2]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 设置随机种子\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载下游任务数据集\n",
    "file_path = 'data2.csv'\n",
    "data2_df = pd.read_csv(file_path)\n",
    "\n",
    "# 使用KFold进行十倍交叉验证\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "# 记录交叉验证的结果\n",
    "fold_accuracies = []\n",
    "fold_conf_matrices = []\n",
    "fold_class_reports = []\n",
    "\n",
    "# 使用上游模型生成增强表示的函数\n",
    "def generate_enhanced_embeddings(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.hidden_states[-1][:, 0, :]\n",
    "    predictions = torch.sigmoid(outputs.logits)\n",
    "    enhanced_embeddings = torch.cat((embeddings, predictions), dim=1)\n",
    "    return enhanced_embeddings\n",
    "\n",
    "# 加载预训练模型和分词器\n",
    "model_path = \"saved_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "config.num_labels = 38  # 手动设置标签数量为 38\n",
    "config.output_hidden_states = True\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, config=config).to(device)\n",
    "\n",
    "# 定义简单的二分类器\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(data2_df)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "    train_df = data2_df.iloc[train_index]\n",
    "    test_df = data2_df.iloc[test_index]\n",
    "\n",
    "    # 创建分类器实例\n",
    "    input_dim = config.hidden_size + config.num_labels\n",
    "    classifier = SimpleClassifier(input_dim).to(device)\n",
    "\n",
    "    # 将模型和分类器的参数放在同一个优化器中，并添加L2正则化\n",
    "    optimizer = optim.Adam(list(model.parameters()) + list(classifier.parameters()), lr=5e-5, weight_decay=1e-4)\n",
    "\n",
    "    # 定义损失函数\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # 记录损失值和测试集准确度\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    # 训练模型和分类器\n",
    "    num_epochs = 500\n",
    "    patience = 100  # 早停法的耐心值\n",
    "    best_accuracy = 0.0\n",
    "    best_epoch = 0\n",
    "    best_model_state = None\n",
    "    best_classifier_state = None\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    model.train()\n",
    "    classifier.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        train_embeddings = generate_enhanced_embeddings(train_df['compound'].tolist(), tokenizer, model)\n",
    "        outputs = classifier(train_embeddings).squeeze()\n",
    "        train_labels = torch.tensor(train_df['label'].values, dtype=torch.float).to(device)\n",
    "        loss = criterion(outputs, train_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # 计算测试集上的准确度\n",
    "        model.eval()\n",
    "        classifier.eval()\n",
    "        with torch.no_grad():\n",
    "            test_embeddings = generate_enhanced_embeddings(test_df['compound'].tolist(), tokenizer, model)\n",
    "            test_outputs = classifier(test_embeddings).squeeze()\n",
    "            test_labels = torch.tensor(test_df['label'].values, dtype=torch.float).to(device)\n",
    "            test_predictions = torch.sigmoid(test_outputs) > 0.5\n",
    "            accuracy = accuracy_score(test_labels.cpu(), test_predictions.cpu())\n",
    "            test_accuracies.append(accuracy)\n",
    "            \n",
    "            # 记录最好的模型\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_epoch = epoch + 1\n",
    "                best_model_state = model.state_dict()\n",
    "                best_classifier_state = classifier.state_dict()\n",
    "                early_stop_counter = 0  # 重置早停计数器\n",
    "            else:\n",
    "                early_stop_counter += 1  # 增加早停计数器\n",
    "        \n",
    "        model.train()\n",
    "        classifier.train()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}, Test Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        # 早停条件满足时停止训练\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    # 在当前fold上评估最好的模型\n",
    "    model.load_state_dict(best_model_state)\n",
    "    classifier.load_state_dict(best_classifier_state)\n",
    "\n",
    "    model.eval()\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        test_embeddings = generate_enhanced_embeddings(test_df['compound'].tolist(), tokenizer, model)\n",
    "        test_outputs = classifier(test_embeddings).squeeze()\n",
    "        test_labels = torch.tensor(test_df['label'].values, dtype=torch.float).to(device)\n",
    "        test_predictions = torch.sigmoid(test_outputs) > 0.5\n",
    "\n",
    "    # 计算混淆矩阵和其他评价指标\n",
    "    conf_matrix = confusion_matrix(test_labels.cpu(), test_predictions.cpu())\n",
    "    class_report = classification_report(test_labels.cpu(), test_predictions.cpu(), target_names=['Class 0', 'Class 1'])\n",
    "    accuracy = accuracy_score(test_labels.cpu(), test_predictions.cpu())\n",
    "\n",
    "    fold_accuracies.append(accuracy)\n",
    "    fold_conf_matrices.append(conf_matrix)\n",
    "    fold_class_reports.append(class_report)\n",
    "\n",
    "    print(\"Best Epoch:\", best_epoch)\n",
    "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "    print(\"\\nClassification Report:\\n\", class_report)\n",
    "    print(f\"Best Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# 计算并输出交叉验证的平均结果\n",
    "average_accuracy = np.mean(fold_accuracies)\n",
    "average_conf_matrix = np.mean(fold_conf_matrices, axis=0)\n",
    "\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(\"Average Accuracy:\", average_accuracy)\n",
    "print(\"Average Confusion Matrix:\\n\", average_conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79403e2c-02f9-461b-9b0b-985ba2c8c14c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b15bb28-3bea-49f2-bec8-addc802a41aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cc00b6-cf43-46a7-b922-d7d76fe3f0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b7d1d1-6012-43db-a88a-6e277a503259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2af06d-72e7-49e3-99b9-bdce6e3dd5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b422e2-14f0-43ae-bf03-94ff5147e41e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79e71b1-8385-4531-b7d1-4a9cc34be262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daef04f7-3665-41e4-b46a-9fbfdb665c93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "storch",
   "language": "python",
   "name": "storch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
